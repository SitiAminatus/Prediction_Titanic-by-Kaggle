{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# Prepocessing\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "from scipy import stats\n",
    "from scipy.stats import chi2_contingency\n",
    "# Modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived  Pclass  \\\n",
      "0              1         0       3   \n",
      "1              2         1       1   \n",
      "2              3         1       3   \n",
      "3              4         1       1   \n",
      "4              5         0       3   \n",
      "..           ...       ...     ...   \n",
      "886          887         0       2   \n",
      "887          888         1       1   \n",
      "888          889         0       3   \n",
      "889          890         1       1   \n",
      "890          891         0       3   \n",
      "\n",
      "                                                  Name     Sex   Age  SibSp  \\\n",
      "0                              Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                               Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                             Allen, Mr. William Henry    male  35.0      0   \n",
      "..                                                 ...     ...   ...    ...   \n",
      "886                              Montvila, Rev. Juozas    male  27.0      0   \n",
      "887                       Graham, Miss. Margaret Edith  female  19.0      0   \n",
      "888           Johnston, Miss. Catherine Helen \"Carrie\"  female   NaN      1   \n",
      "889                              Behr, Mr. Karl Howell    male  26.0      0   \n",
      "890                                Dooley, Mr. Patrick    male  32.0      0   \n",
      "\n",
      "     Parch            Ticket     Fare Cabin Embarked  \n",
      "0        0         A/5 21171   7.2500   NaN        S  \n",
      "1        0          PC 17599  71.2833   C85        C  \n",
      "2        0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3        0            113803  53.1000  C123        S  \n",
      "4        0            373450   8.0500   NaN        S  \n",
      "..     ...               ...      ...   ...      ...  \n",
      "886      0            211536  13.0000   NaN        S  \n",
      "887      0            112053  30.0000   B42        S  \n",
      "888      2        W./C. 6607  23.4500   NaN        S  \n",
      "889      0            111369  30.0000  C148        C  \n",
      "890      0            370376   7.7500   NaN        Q  \n",
      "\n",
      "[891 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "#Read your data\n",
    "df_train = pd.read_csv(r'C:\\Users\\AMINA\\OneDrive\\S2\\S2\\Big Data Analysis and App\\week 1 -2\\train.csv')\n",
    "print(df_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "#Get to know the statistic of your data\n",
    "#https://www.w3schools.com/python/pandas/ref_df_describe.asp\n",
    "\n",
    "print(df_train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAAJdCAYAAADwYjncAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy89olMNAAAACXBIWXMAAA9hAAAPYQGoP6dpAABzyUlEQVR4nO3de1xU1d4/8M+AwwDKgKAwoIiWKeK1UHG8lClCRheVSsujZKYnA0vpWNFTXkvMp8wyxOop6KJHpbykmYKoeArwgl28FEdNQ9MBLwEKMozM+v3hb7aODMplM8MMn/frxSv32mvv/V2LWc2XfVsKIYQAERERETWYk60DICIiInIUTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyoSdu1axcUCgV27dpl61DIwTz99NPo2LGjTY6tUCgwd+5cmxybmqeTJ09CoVAgNTXV1qE4PCZWtZCamgqFQiH9uLq6okuXLoiLi0NhYaGtw7NLCoUCcXFxtg6DrOjgwYN47LHHEBQUBFdXV7Rr1w4jRozAsmXLbB0aUZPF7x/708LWAdiT+fPno1OnTqioqMAPP/yA5ORkbNmyBYcOHYK7u7utwyNqsrKzs3H//fejQ4cOmDJlCjQaDU6dOoXc3Fy8//77mD59utVj+uSTT2A0Gq1+XKL64PeP/WBiVQcjR45E3759AQDPPvssfHx8sGTJEmzcuBFPPvmkjaNr+oxGIyorK+Hq6mrrUMjK3nrrLXh6emLfvn3w8vIyW1dUVCTLMcrKytCyZcta11cqlbIcl8ga+P1jP3gpsAGGDRsGADhx4gTeeecdDBw4ED4+PnBzc0NoaCi+/vrrattkZGRg8ODB8PLyQqtWrdC1a1e89tprZnWWLVuG7t27w93dHa1bt0bfvn2xatUqszp//fUXnnnmGfj5+UGlUqF79+747LPPzOqY7k9au3Yt3nrrLbRv3x6urq4YPnw4jh07Vi22pKQk3HHHHXBzc0P//v3xn//8B0OHDsXQoUPN6un1esyZMwedO3eGSqVCYGAgXn75Zej1erN6pst9K1euRPfu3aFSqbB169Ya+/P06dMYNWoUWrZsCV9fX8ycObPaPsk+HT9+HN27d6+WVAGAr68vgFvfA3LzPUlz586FQqHAkSNH8NRTT6F169YYPHgw3nnnHSgUCvz555/V9pGQkAAXFxf8/fffAMzvsTIYDPD29sakSZOqbVdaWgpXV1f861//kspqOwb0ej1mzpyJtm3bwsPDA4888ghOnz59u+4iuq0bv38AoLi4GDNnzkTHjh2hUqnQvn17TJw4EefPn69xH7/++iuefvpp3HHHHXB1dYVGo8EzzzyDCxcumNW7dOkSZsyYIe3b19cXI0aMwIEDB6Q6R48eRXR0NDQaDVxdXdG+fXuMGzcOJSUljdD6po1nrBrg+PHjAAAfHx+8+eabeOSRRzB+/HhUVlZi9erVePzxx7F582ZERUUBAA4fPoyHHnoIvXr1wvz586FSqXDs2DH8+OOP0j4/+eQTvPDCC3jsscfw4osvoqKiAr/++iv27NmDp556CgBQWFiIAQMGSIlL27Zt8f3332Py5MkoLS3FjBkzzOJctGgRnJyc8K9//QslJSVYvHgxxo8fjz179kh1kpOTERcXhyFDhmDmzJk4efIkRo0ahdatW6N9+/ZSPaPRiEceeQQ//PADpk6dim7duuHgwYN477338N///hcbNmwwO/aOHTuwdu1axMXFoU2bNjXeLHzlyhUMHz4cBQUFeOGFFxAQEIAvv/wSO3bsqO+vh5qQoKAg5OTk4NChQ+jRo4ds+3388cdx1113YeHChRBC4KGHHsLLL7+MtWvXYtasWWZ1165di4iICLRu3brafpRKJUaPHo1169bho48+gouLi7Ruw4YN0Ov1GDduHIC6jYFnn30WX331FZ566ikMHDgQO3bskP5/QNQQN37/XL58GUOGDMFvv/2GZ555Bvfccw/Onz+Pb7/9FqdPn0abNm0s7iMjIwN//PEHJk2aBI1Gg8OHD+Pjjz/G4cOHkZubC4VCAQB47rnn8PXXXyMuLg4hISG4cOECfvjhB/z222+45557UFlZicjISOj1ekyfPh0ajQZ//fUXNm/ejOLiYnh6elqtX5oEQbeVkpIiAIjt27eLc+fOiVOnTonVq1cLHx8f4ebmJk6fPi3Ky8vNtqmsrBQ9evQQw4YNk8ree+89AUCcO3euxmM9+uijonv37reMZ/LkycLf31+cP3/erHzcuHHC09NTimXnzp0CgOjWrZvQ6/VSvffff18AEAcPHhRCCKHX64WPj4/o16+fMBgMUr3U1FQBQNx3331S2ZdffimcnJzEf/7zH7Njr1ixQgAQP/74o1QGQDg5OYnDhw9XawMAERsbKy0vXbpUABBr166VysrKykTnzp0FALFz585b9gk1benp6cLZ2Vk4OzsLrVYrXn75ZbFt2zZRWVkp1Tlx4oQAIFJSUqptD0DMmTNHWp4zZ44AIJ588slqdbVarQgNDTUr27t3rwAgvvjiC6ksJiZGBAUFScvbtm0TAMSmTZvMtn3wwQfFHXfcIS3Xdgz8/PPPAoB4/vnnzeo99dRT1dpDVJPafP/Mnj1bABDr1q2rtr3RaBRCWB5fN39vCSHEv//9bwFA7N69Wyrz9PQ0+//1zX766ScBQKSlpTWgpY6DlwLrIDw8HG3btkVgYCDGjRuHVq1aYf369WjXrh3c3Nyken///TdKSkowZMgQs1OlpssgGzdurPGmWS8vL5w+fRr79u2zuF4IgW+++QYPP/wwhBA4f/689BMZGYmSkhKzYwLApEmTzP4CHzJkCADgjz/+AADs378fFy5cwJQpU9CixfWTmOPHj6/2131aWhq6deuG4OBgs2ObTkvv3LnTrP59992HkJAQi2250ZYtW+Dv74/HHntMKnN3d8fUqVNvuy01fSNGjEBOTg4eeeQR/PLLL1i8eDEiIyPRrl07fPvtt/Xe73PPPVetbOzYscjLy5P+ogeANWvWQKVS4dFHH61xX8OGDUObNm2wZs0aqezvv/9GRkYGxo4dK5XVdgxs2bIFAPDCCy+YHefmM8pEtXGr759vvvkGvXv3xujRo6ttZzrrZMmN31sVFRU4f/48BgwYAADVvrv27NmDM2fOWNyP6YzUtm3bUF5eXq/2ORImVnWQlJSEjIwM7Ny5E0eOHMEff/yByMhIAMDmzZsxYMAAuLq6wtvbG23btkVycrLZ9eWxY8di0KBBePbZZ+Hn54dx48Zh7dq1ZknWK6+8glatWqF///646667EBsba3ap8Ny5cyguLsbHH3+Mtm3bmv2Y7g+5+WbgDh06mC2bkiXTvSam+1E6d+5sVq9FixbVLt0dPXoUhw8frnbsLl26WDx2p06datGz12Lo3Llztf8JdO3atVbbU9PXr18/rFu3Dn///Tf27t2LhIQEXLp0CY899hiOHDlSr31a+nw9/vjjcHJykhIkIQTS0tIwcuRIqNXqGvfVokULREdHY+PGjdK9UuvWrYPBYDBLrGo7Bv788084OTnhzjvvNDsOP9NUH7f6/jl+/Hi9LrFfvHgRL774Ivz8/ODm5oa2bdtKY+rG767Fixfj0KFDCAwMRP/+/TF37lzpD3Pg2jiMj4/H//3f/6FNmzaIjIxEUlJSs7y/CuA9VnXSv39/6amMG/3nP//BI488gnvvvRfLly+Hv78/lEolUlJSzG46d3Nzw+7du7Fz505899132Lp1K9asWYNhw4YhPT0dzs7O6NatG/Lz87F582Zs3boV33zzDZYvX47Zs2dj3rx5UhL2j3/8AzExMRbj7NWrl9mys7OzxXpCiDr3gdFoRM+ePbFkyRKL6wMDA82Wb/yLiAgAXFxc0K9fP/Tr1w9dunTBpEmTkJaWhqefftpi/aqqqhr3ZenzFRAQgCFDhmDt2rV47bXXkJubi4KCArz99tu3jW3cuHH46KOP8P3332PUqFFYu3YtgoOD0bt3b6lOXccAkRxq+v5piCeeeALZ2dmYNWsW+vTpg1atWsFoNOKBBx4w+4P/iSeewJAhQ7B+/Xqkp6fjf//3f/H2229j3bp1GDlyJADg3XffxdNPP42NGzciPT0dL7zwAhITE5Gbm2t2n25zwMRKBt988w1cXV2xbds2qFQqqTwlJaVaXScnJwwfPhzDhw/HkiVLsHDhQvzP//wPdu7cifDwcABAy5YtMXbsWIwdOxaVlZUYM2YM3nrrLSQkJEhPF1VVVUn1GyooKAgAcOzYMdx///1S+dWrV3Hy5EmzRO3OO+/EL7/8guHDh9/yFHN9Yjh06BCEEGb7zc/Pl+0Y1PSYvijOnj0rnUktLi42q2PpCb/bGTt2LJ5//nnk5+djzZo1cHd3x8MPP3zb7e699174+/tjzZo1GDx4MHbs2IH/+Z//MatT2zEQFBQEo9GI48ePm52l4mea5HbnnXfi0KFDddrm77//RmZmJubNm4fZs2dL5UePHrVY39/fH88//zyef/55FBUV4Z577sFbb70lJVYA0LNnT/Ts2ROvv/46srOzMWjQIKxYsQJvvvlm/Rpmp3gpUAbOzs5QKBRmf1mfPHmy2hNyFy9erLZtnz59AEC69HDzY64uLi4ICQmBEAIGgwHOzs6Ijo7GN998Y3EgnTt3rs7x9+3bFz4+Pvjkk09w9epVqXzlypXS5UKTJ554An/99Rc++eSTavu5cuUKysrK6nx8AHjwwQdx5swZs1dUlJeX4+OPP67X/qhp2blzp8UzpKb7kLp27Qq1Wo02bdpg9+7dZnWWL19e5+NFR0fD2dkZ//73v5GWloaHHnqoVu+4cnJywmOPPYZNmzbhyy+/xNWrV80uAwK1HwOmL5wPPvjArM7SpUvr3B6iW4mOjsYvv/yC9evXV1tX05UJ05WMm9ff/PmsqqqqdknP19cXAQEB0vdWaWmp2XcHcC3JcnJyapavzOEZKxlERUVhyZIleOCBB/DUU0+hqKgISUlJ6Ny5M3799Vep3vz587F7925ERUUhKCgIRUVFWL58Odq3b4/BgwcDACIiIqDRaDBo0CD4+fnht99+w4cffoioqCh4eHgAuPb6hJ07dyIsLAxTpkxBSEgILl68iAMHDmD79u0WE7hbcXFxwdy5czF9+nQMGzYMTzzxBE6ePInU1FTceeedZn+VT5gwAWvXrsVzzz2HnTt3YtCgQaiqqsLvv/+OtWvXYtu2bfU6XT1lyhR8+OGHmDhxIvLy8uDv748vv/ySbxR2ENOnT0d5eTlGjx6N4OBgVFZWIjs7G2vWrEHHjh2l+wOfffZZLFq0CM8++yz69u2L3bt347///W+dj+fr64v7778fS5YswaVLl6olR7cyduxYLFu2DHPmzEHPnj3RrVs3s/W1HQN9+vTBk08+ieXLl6OkpAQDBw5EZmamxXfIETXErFmz8PXXX+Pxxx/HM888g9DQUFy8eBHffvstVqxYYXYp20StVuPee+/F4sWLYTAY0K5dO6Snp0vvxTK5dOkS2rdvj8ceewy9e/dGq1atsH37duzbtw/vvvsugGuv1YmLi8Pjjz+OLl264OrVq/jyyy+lEwHNjs2eR7Qjpsdd9+3bV2OdTz/9VNx1111CpVKJ4OBgkZKSIj0SbpKZmSkeffRRERAQIFxcXERAQIB48sknxX//+1+pzkcffSTuvfde4ePjI1QqlbjzzjvFrFmzRElJidnxCgsLRWxsrAgMDBRKpVJoNBoxfPhw8fHHH0t1TK9buPkR2Joea//ggw9EUFCQUKlUon///uLHH38UoaGh4oEHHjCrV1lZKd5++23RvXt3oVKpROvWrUVoaKiYN2+eWZy46ZUKN7K07s8//xSPPPKIcHd3F23atBEvvvii2Lp1K1+34AC+//578cwzz4jg4GDRqlUr4eLiIjp37iymT58uCgsLpXrl5eVi8uTJwtPTU3h4eIgnnnhCFBUV1fi6hVu9uuSTTz4RAISHh4e4cuVKtfU3v27BxGg0isDAQAFAvPnmmxb3XdsxcOXKFfHCCy8IHx8f0bJlS/Hwww+LU6dO8XULVGu1+f4RQogLFy6IuLg40a5dO+Hi4iLat28vYmJipNfyWPr//unTp8Xo0aOFl5eX8PT0FI8//rg4c+aM2edTr9eLWbNmid69ewsPDw/RsmVL0bt3b7F8+XJpP3/88Yd45plnxJ133ilcXV2Ft7e3uP/++8X27dtl7w97oBCiHncwU7NgNBrRtm1bjBkzxuJlDyIiIjLHe6wIwLV3mNycY3/xxRe4ePFitSltiIiIyDKesSIA1+YVnDlzJh5//HH4+PjgwIED+PTTT9GtWzfk5eWZvWCUiIiILOPN6wQA6NixIwIDA/HBBx/g4sWL8Pb2xsSJE7Fo0SImVURERLXEM1ZEREREMuE9VkREREQyYWJFREREJBO7vMfKaDTizJkz8PDwkHVaFSIhBC5duoSAgAA4OdnH3x0cD9RYOB6IrqvteLDLxOrMmTOc6JQa1alTp+xm4lCOB2psHA9E191uPNhlYmWa2uXUqVNQq9Vm6wwGA9LT0xEREQGlUmmL8OwO++y60tJSBAYGSp8xe8DxUHvsj+pu1SccD46N/VGdHOPBLhMr0+ldtVptceC4u7tDrVbzg1JL7LPq7OkSAsdD7bE/qqtNn3A8OCb2R3VyjAf7uGhOREREZAeYWBERERHJhIkVERERkUyYWBERERHJxC5vXq+NHnO3QV91+xsuTy6Kkv7d8dXvar1/03b12cba291uG5WzwOL+1/os/62HbBJjfberyzY1Hbc5MI2H5tZuIqLaMH2XmL4PG4JnrIiIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCZMrIiIiIhk4rDvsaqt+r4HqT7bWfNY9d3OHmJsyHZERESNiWesiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGQie2K1e/duPPzwwwgICIBCocCGDRvM1j/99NNQKBRmPw888IDcYRARERFZneyJVVlZGXr37o2kpKQa6zzwwAM4e/as9PPvf/9b7jCIiIiIrE72xGrkyJF48803MXr06BrrqFQqaDQa6ad169Zyh0FERDa0aNEiKBQKzJgxQyqrqKhAbGwsfHx80KpVK0RHR6OwsNBsu4KCAkRFRcHd3R2+vr6YNWsWrl69auXoieqvhS0OumvXLvj6+qJ169YYNmwY3nzzTfj4+NRYX6/XQ6/XS8ulpaUAAIPBAIPBYFbXtKxyEo0QuWMy9VVz6rObPze3Kyei2tu3bx8++ugj9OrVy6x85syZ+O6775CWlgZPT0/ExcVhzJgx+PHHHwEAVVVViIqKgkajQXZ2Ns6ePYuJEydCqVRi4cKFtmgKUZ1ZPbF64IEHMGbMGHTq1AnHjx/Ha6+9hpEjRyInJwfOzs4Wt0lMTMS8efOqlaenp8Pd3d3iNgv6GmWNuzloTn22ZcsWi+Xl5eVWjoTIsVy+fBnjx4/HJ598gjfffFMqLykpwaeffopVq1Zh2LBhAICUlBR069YNubm5GDBgANLT03HkyBFs374dfn5+6NOnDxYsWIBXXnkFc+fOhYuLi62aRVRrVk+sxo0bJ/27Z8+e6NWrF+68807s2rULw4cPt7hNQkIC4uPjpeXS0lIEBgYiIiICarXarK7BYEBGRgbe2O8EvVHROI1wMCongQV9jc2qzw7NjbRYbjobSkT1Exsbi6ioKISHh5slVnl5eTAYDAgPD5fKgoOD0aFDB+Tk5GDAgAHIyclBz5494efnJ9WJjIzEtGnTcPjwYdx9991WbQtRfdjkUuCN7rjjDrRp0wbHjh2rMbFSqVRQqVTVypVKJZRKpcVt9EYF9FXNI0mQS3Pqs5o+NzWVE9HtrV69GgcOHMC+ffuqrdPpdHBxcYGXl5dZuZ+fH3Q6nVTnxqTKtN60zpL63CrCS/7XsD+uUzmb3xJjqU9q2082T6xOnz6NCxcuwN/f39ahEBFRPZ06dQovvvgiMjIy4OrqarXj1udWkYyMjMYOy66wP4DF/c2XLfVJbW8VkT2xunz5Mo4dOyYtnzhxAj///DO8vb3h7e2NefPmITo6GhqNBsePH8fLL7+Mzp07IzLS8qUZIiJq+vLy8lBUVIR77rlHKquqqsLu3bvx4YcfYtu2baisrERxcbHZWavCwkJoNBoAgEajwd69e832a3pq0FTnZvW5VWTEiBE8Ow32x416zN0G4PqtMZb6pLa3isieWO3fvx/333+/tGz6wMfExCA5ORm//vorPv/8cxQXFyMgIAARERFYsGCBxUt9RERkH4YPH46DBw+alU2aNAnBwcF45ZVXEBgYCKVSiczMTERHRwMA8vPzUVBQAK1WCwDQarV46623UFRUBF9fXwDXzhyo1WqEhIRYPG59bhW51brmiP2BarfBWOqT2vaR7InV0KFDIUTNj+1v27ZN7kMSEZGNeXh4oEePHmZlLVu2hI+Pj1Q+efJkxMfHw9vbG2q1GtOnT4dWq8WAAQMAABEREQgJCcGECROwePFi6HQ6vP7664iNjeUf32Q3bH6PFRERNQ/vvfcenJycEB0dDb1ej8jISCxfvlxa7+zsjM2bN2PatGnQarVo2bIlYmJiMH/+fBtGTVQ3TKyIiKhR7Nq1y2zZ1dUVSUlJt5zyLCgoqMb3zBHZA9mntCEiIiJqrphYEREREcmEiRWRTDjpLBERMbEiksGtJp3dtGkT0tLSkJWVhTNnzmDMmDHSetOks5WVlcjOzsbnn3+O1NRUzJ4929pNICIiGTCxImqgGyedbd26tVRumnR2yZIlGDZsGEJDQ5GSkoLs7Gzk5uYCgDTp7FdffYU+ffpg5MiRWLBgAZKSklBZWWmrJhERUT3xqUCiBrL2pLP1mRvtVvNfNSecG626W/UJ+4mo7phYETWALSadrc/caAv6GgGAj7H/f5wbrbqGzI1GRNcxsSKqJ1tNOlufudHe2O8EvVGBQ3Ob95ycnButulv1SW3nRiOi65hYEdWTrSadrc/caHqjAvoqBZOJ/49zo1XXkLnRiOg63rxOVE+mSWd//vln6adv374YP3689G/TpLMmliadPXjwIIqKiqQ6t5t0loiImi6esSKqJ046S0REN2NiRdSIOOksEVHzwsSKSEacdJaIqHnjPVZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTviCUiKgZ6vjqdwAAlbPA4v42DobIgfCMFREREZFMmFgRERERyYSJFREREZFMmFgRERERyUT2xGr37t14+OGHERAQAIVCgQ0bNpitF0Jg9uzZ8Pf3h5ubG8LDw3H06FG5wyAiIiKyOtkTq7KyMvTu3RtJSUkW1y9evBgffPABVqxYgT179qBly5aIjIxERUWF3KEQERERWZXsr1sYOXIkRo4caXGdEAJLly7F66+/jkcffRQA8MUXX8DPzw8bNmzAuHHj5A6HiIiIyGqseo/ViRMnoNPpEB4eLpV5enoiLCwMOTk51gyFiIiISHZWfUGoTqcDAPj5+ZmV+/n5Sess0ev10Ov10nJpaSkAwGAwwGAwmNU1LauchCwxNwemvmpOfXbz5+Z25URERLVhF29eT0xMxLx586qVp6enw93d3eI2C/oaGzssh9Oc+mzLli0Wy8vLy60cCRERORKrJlYajQYAUFhYCH9/f6m8sLAQffr0qXG7hIQExMfHS8ulpaUIDAxEREQE1Gq1WV2DwYCMjAy8sd8JeqNC3gY4KJWTwIK+xmbVZ4fmRlosN50NJSIiqg+rJladOnWCRqNBZmamlEiVlpZiz549mDZtWo3bqVQqqFSqauVKpRJKpdLiNnqjAvqq5pEkyKU59VlNn5uayomIiGpD9sTq8uXLOHbsmLR84sQJ/Pzzz/D29kaHDh0wY8YMvPnmm7jrrrvQqVMnvPHGGwgICMCoUaPkDoWIiIjIqmRPrPbv34/7779fWjZdwouJiUFqaipefvlllJWVYerUqSguLsbgwYOxdetWuLq6yh0KERERkVXJnlgNHToUQtT8dJlCocD8+fMxf/58uQ9NREREZFOcK5CIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiBosMTER/fr1g4eHB3x9fTFq1Cjk5+eb1amoqEBsbCx8fHzQqlUrREdHo7Cw0KxOQUEBoqKi4O7uDl9fX8yaNQtXr16VNdYec7fJuj+iGzGxIiKiBsvKykJsbCxyc3ORkZEBg8GAiIgIlJWVSXVmzpyJTZs2IS0tDVlZWThz5gzGjBkjra+qqkJUVBQqKyuRnZ2Nzz//HKmpqZg9e7YtmkRUL7JPwkxERM3P1q1bzZZTU1Ph6+uLvLw83HvvvSgpKcGnn36KVatWYdiwYQCAlJQUdOvWDbm5uRgwYADS09Nx5MgRbN++HX5+fujTpw8WLFiAV155BXPnzoWLi4stmkZUJzxjRUREsispKQEAeHt7AwDy8vJgMBgQHh4u1QkODkaHDh2Qk5MDAMjJyUHPnj3h5+cn1YmMjERpaSkOHz5sxeiJ6o9nrIiISFZGoxEzZszAoEGD0KNHDwCATqeDi4sLvLy8zOr6+flBp9NJdW5MqkzrTess0ev10Ov10nJpaSkAwGAwwGAwmNU1LaucRLV1zZGpD9gXgMpZXPuv07X/WuqT2vYTEysiIpJVbGwsDh06hB9++KHRj5WYmIh58+ZVK09PT4e7u7vFbRb0NWLLli2NHZrdyMjIsHUINre4v/mypT4pLy+v1b6YWBERkWzi4uKwefNm7N69G+3bt5fKNRoNKisrUVxcbHbWqrCwEBqNRqqzd+9es/2Znho01blZQkIC4uPjpeXS0lIEBgYiIiICarXarK7BYEBGRgbe2O+EvNkPNKidjsDUHyNGjIBSqbR1ODZlelJU5SSwoK/RYp+YzobeDhMronpKTEzEunXr8Pvvv8PNzQ0DBw7E22+/ja5du0p1Kioq8NJLL2H16tXQ6/WIjIzE8uXLzS53FBQUYNq0adi5cydatWqFmJgYJCYmokULDk+yH0IITJ8+HevXr8euXbvQqVMns/WhoaFQKpXIzMxEdHQ0ACA/Px8FBQXQarUAAK1Wi7feegtFRUXw9fUFcO3MgVqtRkhIiMXjqlQqqFSqauVKpbLGZEFvVDT7ROJGt+qr5kJfpTBbttQnte0j3rxOVE98vJzoutjYWHz11VdYtWoVPDw8oNPpoNPpcOXKFQCAp6cnJk+ejPj4eOzcuRN5eXmYNGkStFotBgwYAACIiIhASEgIJkyYgF9++QXbtm3D66+/jtjYWIvJE1FTxD+JieqJj5cTXZecnAwAGDp0qFl5SkoKnn76aQDAe++9BycnJ0RHR5udwTVxdnbG5s2bMW3aNGi1WrRs2RIxMTGYP3++tZpB1GBMrIhkUtfHywcMGFDj4+XTpk3D4cOHcffdd1c7Tn2fgrpxubniU1DXyfkUFHDtUuDtuLq6IikpCUlJSTXWCQoK4o3lVtZj7jboqxQ4uSjK1qE4BCZWRDKw5uPl9X0KCgC/sP4/PgUl71NQRHQdEysiGVjz8fL6PgWlNypwaG5ko8fXlPEpqOvkfAqKiK5jYkXUQNZ+vLy+T0Hpq/gklAmfgpL3KSgiuo5PBRLVkxACcXFxWL9+PXbs2HHLx8tNLD1efvDgQRQVFUl1bvd4ORERNV08Y0VUT7GxsVi1ahU2btwoPV4OXHus3M3Nzezxcm9vb6jVakyfPr3Gx8sXL14MnU7Hx8uJiOwYEyuieuLj5UREdDMmVkT1xMfLiYjoZrzHioiIiEgmNkms5s6dC4VCYfYTHBxsi1CIiIiIZGOzS4Hdu3fH9u3brwfCCWeJiIjIztksm2nRokWN7+khIiIiskc2S6yOHj2KgIAAuLq6QqvVIjExER06dLBYtyFzo9HtmfqqOfVZTXOgcQ45IiJqCJskVmFhYUhNTUXXrl1x9uxZzJs3D0OGDMGhQ4fg4eFRrX5D5kaj2mtOfVbTU3icG42IiBrCJonVyJEjpX/36tULYWFhCAoKwtq1azF58uRq9RsyNxrdnmmusObUZzXNmce50YiIqCGaxB3jXl5e6NKlC44dO2ZxfUPmRqPaa059VtPnhnOjERFRQzSJ91hdvnwZx48fh7+/v61DISIiIqo3myRW//rXv5CVlYWTJ08iOzsbo0ePhrOzM5588klbhENEREQkC5tcCjx9+jSefPJJXLhwAW3btsXgwYORm5uLtm3b2iIcIiIiIlnYJLFavXq1LQ5LRERE1KiaxD1WRERERI6AiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcnEZolVUlISOnbsCFdXV4SFhWHv3r22CoXI5jgeiK7jeCB7ZpPEas2aNYiPj8ecOXNw4MAB9O7dG5GRkSgqKrJFOEQ2xfFAdB3HA9k7myRWS5YswZQpUzBp0iSEhIRgxYoVcHd3x2effWaLcIhsiuOB6DqOB7J3Lax9wMrKSuTl5SEhIUEqc3JyQnh4OHJycixuo9frodfrpeWSkhIAwMWLF2EwGMzqGgwGlJeXo4XBCVVGRSO0wPG0MAqUlxubVZ9duHDBYvmlS5cAAEIIq8Rh7fFQU7ubC1N/XLhwAUql0tbh2FSLq2XX/vv/x7+lPnHk8dDcxwLA/z/cSM7xYPXE6vz586iqqoKfn59ZuZ+fH37//XeL2yQmJmLevHnVyjt16tQoMTZHT9k6ACtr8+6t11+6dAmenp6NHoe1x8Pt2k3N0+3GvyOOhzb/W/84HRX//3BNQ8eD1ROr+khISEB8fLy0bDQacfHiRfj4+EChMD/DUlpaisDAQJw6dQpqtdraodol9tl1QghcunQJAQEBtg6lRhwP9cf+qO5WfcLx4NjYH9XJMR6snli1adMGzs7OKCwsNCsvLCyERqOxuI1KpYJKpTIr8/LyuuVx1Go1Pyh1xD67xhp/mZtwPNgG+6O6mvqE48HxsT+qa8h4sPrN6y4uLggNDUVmZqZUZjQakZmZCa1Wa+1wiGyK44HoOo4HcgQ2uRQYHx+PmJgY9O3bF/3798fSpUtRVlaGSZMm2SIcIpvieCC6juOB7J1NEquxY8fi3LlzmD17NnQ6Hfr06YOtW7dWu2GxPlQqFebMmVPt1DDVjH1mWxwP1sP+qK6p9QnHg/WwP6qTo08UwlrP0RIRERE5OM4VSERERCQTJlZEREREMmFiRURERCQTJlZEREREMnGoxCopKQkdO3aEq6srwsLCsHfvXluHZDNz586FQqEw+wkODpbWV1RUIDY2Fj4+PmjVqhWio6OrvZSvoKAAUVFRcHd3h6+vL2bNmoWrV69auyl0G3X93KelpSE4OBiurq7o2bMntmzZYqVIraMu/ZGamlptnLi6ulox2sa1e/duPPzwwwgICIBCocCGDRtuu82uXbtwzz33QKVSoXPnzkhNTW30OBtbffrBkSUmJqJfv37w8PCAr68vRo0ahfz8fFuHZTPJycno1auX9FJQrVaL77//vt77c5jEas2aNYiPj8ecOXNw4MAB9O7dG5GRkSgqKrJ1aDbTvXt3nD17Vvr54YcfpHUzZ87Epk2bkJaWhqysLJw5cwZjxoyR1ldVVSEqKgqVlZXIzs7G559/jtTUVMyePdsWTaEa1PVzn52djSeffBKTJ0/GTz/9hFGjRmHUqFE4dOiQlSNvHPX5/4BarTYbJ3/++acVI25cZWVl6N27N5KSkmpV/8SJE4iKisL999+Pn3/+GTNmzMCzzz6Lbdu2NXKkjauu/eDosrKyEBsbi9zcXGRkZMBgMCAiIgJlZWW2Ds0m2rdvj0WLFiEvLw/79+/HsGHD8Oijj+Lw4cP126FwEP379xexsbHSclVVlQgICBCJiYk2jMp25syZI3r37m1xXXFxsVAqlSItLU0q++233wQAkZOTI4QQYsuWLcLJyUnodDqpTnJyslCr1UKv1zdq7FR7df3cP/HEEyIqKsqsLCwsTPzzn/9s1Ditpa79kZKSIjw9Pa0UnW0BEOvXr79lnZdffll0797drGzs2LEiMjKyESOzrtr0Q3NTVFQkAIisrCxbh9JktG7dWvzf//1fvbZ1iDNWlZWVyMvLQ3h4uFTm5OSE8PBw5OTk2DAy2zp69CgCAgJwxx13YPz48SgoKAAA5OXlwWAwmPVXcHAwOnToIPVXTk4OevbsafZSvsjISJSWltY/iydZ1edzn5OTY1YfuPZ7dYRxUt//D1y+fBlBQUEIDAxs2F+pDsCRPx9Us5KSEgCAt7e3jSOxvaqqKqxevRplZWX1nkbJIRKr8+fPo6qqqtqbef38/KDT6WwUlW2FhYUhNTUVW7duRXJyMk6cOIEhQ4bg0qVL0Ol0cHFxqTZR6Y39pdPpLPanaR3ZXn0+9zX9Xh3hd1qf/ujatSs+++wzbNy4EV999RWMRiMGDhyI06dPWyPkJqemz0dpaSmuXLlio6ioMRmNRsyYMQODBg1Cjx49bB2OzRw8eBCtWrWCSqXCc889h/Xr1yMkJKRe+7LJlDbU+EaOHCn9u1evXggLC0NQUBDWrl0LNzc3G0ZG1HRotVqzv0oHDhyIbt264aOPPsKCBQtsGBmRdcTGxuLQoUNm9+A2R127dsXPP/+MkpISfP3114iJiUFWVla9kiuHOGPVpk0bODs7V3uqrbCwEBqNxkZRNS1eXl7o0qULjh07Bo1Gg8rKShQXF5vVubG/NBqNxf40rSPbq8/nvqbfqyP8TuX4/4BSqcTdd9+NY8eONUaITV5Nnw+1Ws0/yBxQXFwcNm/ejJ07d6J9+/a2DsemXFxc0LlzZ4SGhiIxMRG9e/fG+++/X699OURi5eLigtDQUGRmZkplRqMRmZmZ9b5G6mguX76M48ePw9/fH6GhoVAqlWb9lZ+fj4KCAqm/tFotDh48aPY0VUZGBtRqdb1Pj5K86vO512q1ZvWBa79XRxgncvx/oKqqCgcPHoS/v39jhdmkOfLng64TQiAuLg7r16/Hjh070KlTJ1uH1OQYjUbo9fr6bSzvffS2s3r1aqFSqURqaqo4cuSImDp1qvDy8jJ7qq05eemll8SuXbvEiRMnxI8//ijCw8NFmzZtRFFRkRBCiOeee0506NBB7NixQ+zfv19otVqh1Wql7a9evSp69OghIiIixM8//yy2bt0q2rZtKxISEmzVJLLgdp/7CRMmiFdffVWq/+OPP4oWLVqId955R/z2229izpw5QqlUioMHD9qqCbKqa3/MmzdPbNu2TRw/flzk5eWJcePGCVdXV3H48GFbNUFWly5dEj/99JP46aefBACxZMkS8dNPP4k///xTCCHEq6++KiZMmCDV/+OPP4S7u7uYNWuW+O2330RSUpJwdnYWW7dutVUTZHG7fmhupk2bJjw9PcWuXbvE2bNnpZ/y8nJbh2YTr776qsjKyhInTpwQv/76q3j11VeFQqEQ6enp9dqfwyRWQgixbNky0aFDB+Hi4iL69+8vcnNzbR2SzYwdO1b4+/sLFxcX0a5dOzF27Fhx7Ngxaf2VK1fE888/L1q3bi3c3d3F6NGjxdmzZ832cfLkSTFy5Ejh5uYm2rRpI1566SVhMBis3RS6jVt97u+77z4RExNjVn/t2rWiS5cuwsXFRXTv3l189913Vo64cdWlP2bMmCHV9fPzEw8++KA4cOCADaJuHDt37hQAqv2Y+iAmJkbcd9991bbp06ePcHFxEXfccYdISUmxetxyu10/NDeW+gKAQ/yu6+OZZ54RQUFBwsXFRbRt21YMHz683kmVEEIohBCiIafLiIiIiOgah7jHioiIiKgpYGJFRHatY8eOePrpp6Vl0/x/+/fvt11QRA6MY+zWmFjZqeXLl0OhUCAsLMzWoRA1moMHD+Kxxx5DUFAQXF1d0a5dO4wYMQLLli2TZf9GoxFffPEFwsLC4O3tDQ8PD3Tp0gUTJ05Ebm6uLMcgksvNk4a7urqiS5cuiIuLq/aaDLIdviDUTq1cuRIdO3bE3r17cezYMXTu3NnWIRHJKjs7G/fffz86dOiAKVOmQKPR4NSpU8jNzcX777+P6dOnA7j2qhAnp/r9jfjCCy8gKSkJjz76KMaPH48WLVogPz8f33//Pe644w4MGDBAziYRyWL+/Pno1KkTKioq8MMPPyA5ORlbtmzBoUOH4O7ubuvwmj0mVnboxIkTyM7Oxrp16/DPf/4TK1euxJw5c2wdFpGs3nrrLXh6emLfvn3Vpl+68f1qKpWqXvsvLCzE8uXLMWXKFHz88cdm65YuXYpz587Va79EjW3kyJHo27cvAODZZ5+Fj48PlixZgo0bN+LJJ5+s1z6NRiMqKyvh6uoqZ6jNEi8F2qGVK1eidevWiIqKwmOPPYaVK1dWq3PhwgVMmDABarUaXl5eiImJwS+//AKFQoHU1FSzur///jsee+wxeHt7w9XVFX379sW3335rpdYQWXb8+HF07969WlIFAL6+vtK/b77HyqS8vBz//Oc/4ePjA7VajYkTJ+Lvv/+W1p84cQJCCAwaNKjatgqFwuwYpkswu3fvvuU+iWxh2LBhAK59pt955x0MHDgQPj4+cHNzQ2hoKL7++utq2ygUCsTFxWHlypXo3r07VCoVtm7dCgD466+/MHnyZAQEBEClUqFTp06YNm0aKisrzfah1+sRHx+Ptm3bomXLlhg9ejT/IAHPWNmllStXYsyYMXBxccGTTz6J5ORk7Nu3D/369QNw7S+Phx9+GHv37sW0adMQHByMjRs3IiYmptq+Dh8+jEGDBqFdu3Z49dVX0bJlS6xduxajRo3CN998g9GjR1u7eUQAgKCgIOTk5ODQoUP1mhw2Li4OXl5emDt3LvLz85GcnIw///wTu3btgkKhQFBQEAAgLS0Njz/+eK0uodxun0S2cPz4cQCAj48P3nzzTTzyyCMYP348KisrsXr1ajz++OPYvHkzoqKizLbbsWMH1q5di7i4OLRp0wYdO3bEmTNn0L9/fxQXF2Pq1KkIDg7GX3/9ha+//hrl5eVwcXGRtp8+fTpat26NOXPm4OTJk1i6dCni4uKwZs0aq7a/yZHrBVtkHfv37xcAREZGhhBCCKPRKNq3by9efPFFqc4333wjAIilS5dKZVVVVWLYsGHVXgI3fPhw0bNnT1FRUSGVGY1GMXDgQHHXXXc1enuIapKeni6cnZ2Fs7Oz0Gq14uWXXxbbtm0TlZWVZvWCgoLMXvSYkpIiAIjQ0FCzuosXLxYAxMaNG6WyiRMnCgCidevWYvTo0dIb6W9Wl30SNRbT53D79u3i3Llz4tSpU2L16tXCx8dHuLm5idOnT1d7e3plZaXo0aOHGDZsmFk5AOHk5FRtloGJEycKJycnsW/fvmrHNxqNZnGEh4dLZUIIMXPmTOHs7CyKi4vlarJd4qVAO7Ny5Ur4+fnh/vvvB3DtdO7YsWOxevVqVFVVAQC2bt0KpVKJKVOmSNs5OTkhNjbWbF8XL17Ejh078MQTT+DSpUs4f/48zp8/jwsXLiAyMhJHjx7FX3/9Zb3GEd1gxIgRyMnJwSOPPIJffvkFixcvRmRkJNq1a1erS9VTp06FUqmUlqdNm4YWLVpgy5YtUllKSgo+/PBDdOrUCevXr8e//vUvdOvWDcOHD7f42a/NPokaW3h4ONq2bYvAwECMGzcOrVq1wvr169GuXTuzybL//vtvlJSUYMiQIThw4EC1/dx3331mc78ajUZs2LABDz/8sHQP141uPis7depUs7IhQ4agqqoKf/75pxzNtFtMrOxIVVUVVq9ejfvvvx8nTpzAsWPHcOzYMYSFhaGwsFCaPPXPP/+Ev79/tUsbNz85eOzYMQgh8MYbb6Bt27ZmP6ab4W+8SZjI2vr164d169bh77//xt69e5GQkIBLly7hsccew5EjR2657V133WW23KpVK/j7++PkyZNSmekPjry8PJw/fx4bN27EyJEjsWPHDowbN65e+yRqbElJScjIyMDOnTtx5MgR/PHHH4iMjAQAbN68GQMGDICrqyu8vb3Rtm1bJCcno6SkpNp+bp58+dy5cygtLa31pfcOHTqYLbdu3RoAmv19h7zHyo7s2LEDZ8+exerVq7F69epq61euXImIiIha789oNAIA/vWvf0mD8mZ8jQM1BS4uLujXrx/69euHLl26YNKkSUhLS5P1aVgfHx888sgjeOSRRzB06FBkZWXhzz//lO7FImoq+vfvb/GM0n/+8x888sgjuPfee7F8+XL4+/tDqVQiJSUFq1atqlb/xrNb9eHs7GyxXDTzmfKYWNmRlStXwtfXF0lJSdXWrVu3DuvXr8eKFSsQFBSEnTt3ory83Oys1bFjx8y2ueOOOwAASqUS4eHhjRs8kUxMXyhnz569Zb2jR49Kl8wB4PLlyzh79iwefPDBWh0jKysLZ8+eNUusGrJPosb2zTffwNXVFdu2bTN7DUlKSkqttm/bti3UajUOHTrUWCE2C7wUaCeuXLmCdevW4aGHHsJjjz1W7ScuLg6XLl3Ct99+i8jISBgMBnzyySfS9kajsVpC5uvri6FDh+Kjjz6y+CXFx2bJlnbu3GnxL1/T/Uxdu3a95fYff/wxDAaDtJycnIyrV69i5MiRAACdTmfxcmJlZSUyMzPh5ORU7Yzt7fZJZEvOzs5QKBTS/bYAcPLkSWzYsKFW2zs5OWHUqFHYtGmTxelqmvuZqNriGSs78e233+LSpUt45JFHLK4fMGAA2rZti5UrV2L9+vXo378/XnrpJRw7dgzBwcH49ttvcfHiRQDmNyAmJSVh8ODB6NmzJ6ZMmYI77rgDhYWFyMnJwenTp/HLL79YpX1EN5s+fTrKy8sxevRoBAcHo7KyEtnZ2VizZg06duyISZMm3XL7yspKDB8+HE888QTy8/OxfPlyDB48WBpDp0+fRv/+/TFs2DAMHz4cGo0GRUVF+Pe//41ffvkFM2bMQJs2beq0TyJbioqKwpIlS/DAAw/gqaeeQlFREZKSktC5c2f8+uuvtdrHwoULkZ6ejvvuuw9Tp05Ft27dcPbsWaSlpeGHH36w+F45uoltH0qk2nr44YeFq6urKCsrq7HO008/LZRKpTh//rw4d+6ceOqpp4SHh4fw9PQUTz/9tPjxxx8FALF69Wqz7Y4fPy4mTpwoNBqNUCqVol27duKhhx4SX3/9dWM3i6hG33//vXjmmWdEcHCwaNWqlXBxcRGdO3cW06dPF4WFhVK9ml63kJWVJaZOnSpat24tWrVqJcaPHy8uXLgg1SstLRXvv/++iIyMFO3btxdKpVJ4eHgIrVYrPvnkE7PHyGu7T6LGZPocWnoVgsmnn34q7rrrLqFSqURwcLBISUkRc+bMETd/3QMQsbGxFvfx559/iokTJ4q2bdsKlUol7rjjDhEbGyv0ev0t49i5c6cAIHbu3Nmwhto5hRA8t9dcbNiwAaNHj8YPP/xg8W3TRGRZamoqJk2ahH379lm8aZiIyIT3WDmoK1eumC1XVVVh2bJlUKvVuOeee2wUFRERkWPjPVYOavr06bhy5Qq0Wi30ej3WrVuH7OxsLFy4sMGP2BIREZFlTKwc1LBhw/Duu+9i8+bNqKioQOfOnbFs2TLExcXZOjQiIiKHxXusiIiIiGTCe6yIiIiIZMLEioiIiEgmdnmPldFoxJkzZ+Dh4VFttm2ihhBC4NKlSwgICICTk3383cHxQI2F44HoutqOB7tMrM6cOYPAwEBbh0EO7NSpU2jfvr2tw6gVjgdqbBwPRNfdbjzYZWLl4eEB4Frj1Gq1VG4wGJCeno6IiAgolUpbhedwmlO/lpaWIjAwUPqM2YOaxgPQvH531tKc+pTjwf6wjY2ntuPBLhMr0+ldtVpdLbFyd3eHWq122A+ULTTHfrWnSwg1jQegef7uGltz7FOOB/vBNja+240H+7hoTkRERGQHmFgRNcBff/2Ff/zjH/Dx8YGbmxt69uyJ/fv3S+uFEJg9ezb8/f3h5uaG8PBwHD161GwfFy9exPjx46FWq+Hl5YXJkyfj8uXL1m4KERHJgIkVUT39/fffGDRoEJRKJb7//nscOXIE7777Llq3bi3VWbx4MT744AOsWLECe/bsQcuWLREZGYmKigqpzvjx43H48GFkZGRg8+bN2L17N6ZOnWqLJhERUQPZ5T1WRE3B22+/jcDAQKSkpEhlnTp1kv4thMDSpUvx+uuv49FHHwUAfPHFF/Dz88OGDRswbtw4/Pbbb9i6dSv27duHvn37AgCWLVuGBx98EO+88w4CAgKs2ygiImoQnrGSUcdXv5N+yPF9++236Nu3Lx5//HH4+vri7rvvxieffCKtP3HiBHQ6HcLDw6UyT09PhIWFIScnBwCQk5MDLy8vKakCgPDwcDg5OWHPnj2yxdpj7jZ+Lon+vx5zt9k6BHJgPGNFVE9//PEHkpOTER8fj9deew379u3DCy+8ABcXF8TExECn0wEA/Pz8zLbz8/OT1ul0Ovj6+pqtb9GiBby9vaU6N9Pr9dDr9dJyaWkpgGtPyhgMBrO6pmWVkzBbpvoz9WFz6Mvm0EYiuTGxIqono9GIvn37YuHChQCAu+++G4cOHcKKFSsQExPTaMdNTEzEvHnzqpWnp6fD3d3d4jYL+hoBAFu2bGm0uJqbjIwMW4fQ6MrLy20dApHdYWJFVE/+/v4ICQkxK+vWrRu++eYbAIBGowEAFBYWwt/fX6pTWFiIPn36SHWKiorM9nH16lVcvHhR2v5mCQkJiI+Pl5ZNL62LiIiw+N6ejIwMvLHfCXqjAofmRtavsSQx9emIESMc9j1BJqazoURUe0ysiOpp0KBByM/PNyv773//i6CgIADXbmTXaDTIzMyUEqnS0lLs2bMH06ZNAwBotVoUFxcjLy8PoaGhAIAdO3bAaDQiLCzM4nFVKhVUKlW1cqVSWeMXvd6ogL5K4fCJgDXdqr8dhaO3j6gxMLEiqqeZM2di4MCBWLhwIZ544gns3bsXH3/8MT7++GMA197OO2PGDLz55pu466670KlTJ7zxxhsICAjAqFGjAFw7w/XAAw9gypQpWLFiBQwGA+Li4jBu3Dg+EUhEZIeYWBHVU79+/bB+/XokJCRg/vz56NSpE5YuXYrx48dLdV5++WWUlZVh6tSpKC4uxuDBg7F161a4urpKdVauXIm4uDgMHz4cTk5OiI6OxgcffGCLJhERUQMxsSJqgIceeggPPfRQjesVCgXmz5+P+fPn11jH29sbq1ataozwiIjIyvgeKyIiIiKZMLEiIiIikgkTKyIiIiKZMLFqYjglDhERkf1iYkVERLL466+/8I9//AM+Pj5wc3NDz549sX//fmm9EAKzZ8+Gv78/3NzcEB4ejqNHj5rt4+LFixg/fjzUajW8vLwwefJkXL582dpNIao3JlZERNRgf//9NwYNGgSlUonvv/8eR44cwbvvvovWrVtLdRYvXowPPvgAK1aswJ49e9CyZUtERkaioqJCqjN+/HgcPnwYGRkZ2Lx5M3bv3o2pU6faoklE9cLXLVjRjZf4Ti6KsmEkRETyevvttxEYGIiUlBSprFOnTtK/hRBYunQpXn/9dTz66KMAgC+++AJ+fn7YsGEDxo0bh99++w1bt27Fvn370LdvXwDAsmXL8OCDD+Kdd97hS3PJLjCxIiKiBvv2228RGRmJxx9/HFlZWWjXrh2ef/55TJkyBQBw4sQJ6HQ6hIeHS9t4enoiLCwMOTk5GDduHHJycuDl5SUlVQAQHh4OJycn7NmzB6NHj652XL1eD71eLy2b5jc0GAwwGAxmdU3LKidRbZ2jMLXLUdsH2K6NtT0eEysiImqwP/74A8nJyYiPj8drr72Gffv24YUXXoCLiwtiYmKg0+kAAH5+fmbb+fn5Set0Oh18fX3N1rdo0QLe3t5SnZslJiZi3rx51crT09Ph7u5ucZsFfY3YsmVLndtoTzIyMmwdQqOzdhvLy8trVY+JFRERNZjRaETfvn2xcOFCAMDdd9+NQ4cOYcWKFYiJiWm04yYkJCA+Pl5aLi0tRWBgICIiIqBWq83qGgwGZGRk4I39Tsib/UCjxWRLpjaOGDHCYSfRtlUbTWdDb4eJFRERNZi/vz9CQkLMyrp164ZvvvkGAKDRaAAAhYWF8Pf3l+oUFhaiT58+Up2ioiKzfVy9ehUXL16Utr+ZSqWCSqWqVq5UKmv80tUbFQ6bdJjcqv2OwtptrO2x+FQgERE12KBBg5Cfn29W9t///hdBQUEArt3IrtFokJmZKa0vLS3Fnj17oNVqAQBarRbFxcXIy8uT6uzYsQNGoxFhYWFWaAVRw/GMFRERNdjMmTMxcOBALFy4EE888QT27t2Ljz/+GB9//DGAaxOSz5gxA2+++SbuuusudOrUCW+88QYCAgIwatQoANfOcD3wwAOYMmUKVqxYAYPBgLi4OIwbN45PBJLdYGJFREQN1q9fP6xfvx4JCQmYP38+OnXqhKVLl2L8+PFSnZdffhllZWWYOnUqiouLMXjwYGzduhWurq5SnZUrVyIuLg7Dhw+Hk5MToqOj8cEHH9iiSUT1wsSKiIhk8dBDD+Ghhx6qcb1CocD8+fMxf/78Gut4e3tj1apVjREekVXwHisiIiIimTCxIiIiIpIJEysiIiIimTR6YrVo0SLpaRCTiooKxMbGwsfHB61atUJ0dDQKCwsbOxQiIiKiRtWoidW+ffvw0UcfoVevXmblM2fOxKZNm5CWloasrCycOXMGY8aMacxQiIiIiBpdoyVWly9fxvjx4/HJJ5+gdevWUnlJSQk+/fRTLFmyBMOGDUNoaChSUlKQnZ2N3NzcxgqHiIiIqNE1WmIVGxuLqKgos5nMASAvLw8Gg8GsPDg4GB06dEBOTk5jhUNERETU6BrlPVarV6/GgQMHsG/fvmrrdDodXFxc4OXlZVZ+4wznN9Pr9dDr9dKyaSJEg8EAg8EglZv+fWOZNamcRbVY6rL+xjq2aoMltu5Xa2oObSQiosYje2J16tQpvPjii8jIyDB7m25DJCYmYt68edXK09PT4e7uXq08IyNDluPW1eL+1/+9ZcuWOq+/sU5N623JVv1qTeXl5bYOgYiI7JjsiVVeXh6Kiopwzz33SGVVVVXYvXs3PvzwQ2zbtg2VlZUoLi42O2tVWFhY4+zlCQkJiI+Pl5ZLS0sRGBiIiIgIqNVqqdxgMCAjIwMjRoywyazePeZus1h+aG5ktfWmspr2UdN6W7B1v1qT6WwoERFRfcieWA0fPhwHDx40K5s0aRKCg4PxyiuvIDAwEEqlEpmZmYiOjgYA5Ofno6CgQJrh/GYqlQoqlapauVKptPhFX1N5Y9NXKSyWm2K5cX1N8ZnqNMUExlb9ak2O3j4iImpcsidWHh4e6NGjh1lZy5Yt4ePjI5VPnjwZ8fHx8Pb2hlqtxvTp06HVajFgwAC5wyEiIiKyGptMwvzee+9Js5br9XpERkZi+fLltgiFiIiISDZWSax27dpltuzq6oqkpCQkJSVZ4/BEREREVsG5AomIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCZMrIiIiIhkYpPXLZC5jq9+Z+sQiIiISAY8Y0VEREQkEyZWRDJZtGgRFAoFZsyYIZVVVFQgNjYWPj4+aNWqFaKjo1FYWGi2XUFBAaKiouDu7g5fX1/MmjULV69etXL0REQkB14KtAM3Xio8uSjKhpFQTfbt24ePPvoIvXr1MiufOXMmvvvuO6SlpcHT0xNxcXEYM2YMfvzxRwDXJiiPioqCRqNBdnY2zp49i4kTJ0KpVGLhwoW2aAoRETUAz1gRNdDly5cxfvx4fPLJJ2jdurVUXlJSgk8//RRLlizBsGHDEBoaipSUFGRnZyM3NxcAkJ6ejiNHjuCrr75Cnz59MHLkSCxYsABJSUmorKy0VZOIiKiemFgRNVBsbCyioqIQHh5uVp6XlweDwWBWHhwcjA4dOiAnJwcAkJOTg549e8LPz0+qExkZidLSUhw+fNg6DSAiItnwUqCd4uXBpmH16tU4cOAA9u3bV22dTqeDi4sLvLy8zMr9/Pyg0+mkOjcmVab1pnWW6PV66PV6abm0tBQAYDAYYDAYzOqallVOwmyZ6s/Uh82hL5tDG4nkxsSKqJ5OnTqFF198ERkZGXB1dbXacRMTEzFv3rxq5enp6XB3d7e4zYK+RgDAli1bGjW25iQjI8PWITS68vJyW4dAZHeYWBHVU15eHoqKinDPPfdIZVVVVdi9ezc+/PBDbNu2DZWVlSguLjY7a1VYWAiNRgMA0Gg02Lt3r9l+TU8NmurcLCEhAfHx8dJyaWkpAgMDERERAbVabVbXYDAgIyMDb+x3gt6owKG5kQ1qM13v0xEjRkCpVNo6nEZlOhtKRLXHxIqonoYPH46DBw+alU2aNAnBwcF45ZVXEBgYCKVSiczMTERHRwMA8vPzUVBQAK1WCwDQarV46623UFRUBF9fXwDXzoSo1WqEhIRYPK5KpYJKpapWrlQqa/yi1xsV0FcpHD4RsKZb9bejcPT2ETUGJlZE9eTh4YEePXqYlbVs2RI+Pj5S+eTJkxEfHw9vb2+o1WpMnz4dWq0WAwYMAABEREQgJCQEEyZMwOLFi6HT6fD6668jNjbWYvJERERNGxMrokb03nvvwcnJCdHR0dDr9YiMjMTy5cul9c7Ozti8eTOmTZsGrVaLli1bIiYmBvPnz7dh1EREVF983QKRjHbt2oWlS5dKy66urkhKSsLFixdRVlaGdevWVbt3KigoCFu2bEF5eTnOnTuHd955By1a8G8esm+ciYCaKyZWREQkq1vNRLBp0yakpaUhKysLZ86cwZgxY6T1ppkIKisrkZ2djc8//xypqamYPXu2tZtAVG9MrIiISDaciYCaOyZWREQkG85EQM0db+QgIiJZ2NNMBI76VvnmMDOArdpY2+MxsSIiogazt5kIHH0WguYwM4C121jbmQiYWBERUYPZ20wEebMfaFB7m6rmMDOArdpY25kImFgREVGD2dtMBI6adJg0l5kBrNnG2h6LiZUD6Pjqd9K/Ty6KsmEkRNRccSYComuYWBERkVVwJgJqDphYERFRo9i1a5fZsmkmgqSkpBq3Mc1EQGSv+B4rIiIiIpkwsSIiIiKSCS8FWsGNN5c35jY17YM3tBMREVkHz1gRERERyYSJFREREZFMmFgRERERyYSJFREREZFMmFgRERERyYSJFREREZFMmFgRERERyYSJFREREZFMHPIFoT3mboO+SgGg6b4cU44XgBIREVHTwjNWRERERDJhYkVEREQkEyZWRERERDJhYkVEREQkEyZW9dTx1e94AzoRERGZkT2xSkxMRL9+/eDh4QFfX1+MGjUK+fn5ZnUqKioQGxsLHx8ftGrVCtHR0SgsLJQ7FCIiIiKrkj2xysrKQmxsLHJzc5GRkQGDwYCIiAiUlZVJdWbOnIlNmzYhLS0NWVlZOHPmDMaMGSN3KERERERWJft7rLZu3Wq2nJqaCl9fX+Tl5eHee+9FSUkJPv30U6xatQrDhg0DAKSkpKBbt27Izc3FgAED5A6JiIiIyCoa/R6rkpISAIC3tzcAIC8vDwaDAeHh4VKd4OBgdOjQATk5OY0dDhEREVGjadQ3rxuNRsyYMQODBg1Cjx49AAA6nQ4uLi7w8vIyq+vn5wedTmdxP3q9Hnq9XlouLS0FABgMBhgMBqnc9G+Vk6hWJjeVs6i2f1NZY5LaWMOxLMXT0D4wbd9YfdmUNIc2EhFR42nUxCo2NhaHDh3CDz/80KD9JCYmYt68edXK09PT4e7uXq18QV+j9O8tW7Y06Ng1Wdy/+v5NZY3JdLyajmUpHrn6ICMjQ5b9NGXl5eW2DoGIiOxYoyVWcXFx2Lx5M3bv3o327dtL5RqNBpWVlSguLjY7a1VYWAiNRmNxXwkJCYiPj5eWS0tLERgYiIiICKjVaqncYDAgIyMDb+x3gt54ba7AQ3MjZW7ZNT3mbqu2f1NZYzIdr6ZjWYqnoX1g6tcRI0ZAqVQ2aF9NnelsKBERUX3InlgJITB9+nSsX78eu3btQqdOnczWh4aGQqlUIjMzE9HR0QCA/Px8FBQUQKvVWtynSqWCSqWqVq5UKi1+0euNCmkS5sZKBCzt31TWmEzHq+lYluKRqw9q6m9H4ujtIyKixiV7YhUbG4tVq1Zh48aN8PDwkO6b8vT0hJubGzw9PTF58mTEx8fD29sbarUa06dPh1ar5ROBREREZNdkfyowOTkZJSUlGDp0KPz9/aWfNWvWSHXee+89PPTQQ4iOjsa9994LjUaDdevWyR0KUaOS62W4BQUFiIqKgru7O3x9fTFr1ixcvXrVmk0hIiKZNMqlwNtxdXVFUlISkpKS5D483caN0/CcXBRV5/V0nelluP369cPVq1fx2muvISIiAkeOHEHLli0BXHsZ7nfffYe0tDR4enoiLi4OY8aMwY8//ggAqKqqQlRUFDQaDbKzs3H27FlMnDgRSqUSCxcutGXziIioHhr1qUAiRybHy3DT09Nx5MgRbN++HX5+fujTpw8WLFiAV155BXPnzoWLi4stmkZERPXESZiJZFKfl+Hm5OSgZ8+e8PPzk+pERkaitLQUhw8ftmL0NTNNOM5Jx4mIbo9nrOxMY3251Wa/PeZug75KwUuEFtT3Zbg6nc4sqTKtN62zpLYvzDWVAddfmlufF6De+DJavkCVL8wloltjYkUkA7lehlsbdX1hLnD9pbn1eVnsjS+jbawX7tojvjDXXGJiItatW4fff/8dbm5uGDhwIN5++2107dpVqlNRUYGXXnoJq1evhl6vR2RkJJYvX272x0VBQQGmTZuGnTt3olWrVoiJiUFiYiJatODXFdkHflKJGqghL8PVaDTYu3ev2f5MTw029IW5QPWX5tbnZbE3voy2sV64a0/4wlzL+DAH0TVMrIjqSY6X4Wq1Wrz11lsoKiqCr68vgGtnQtRqNUJCQiwet64vzAWuvzS3PonAjS+jdfREoi74wlxzfJiD6BrevE5UT7Gxsfjqq6+watUq6WW4Op0OV65cAQCzl+Hu3LkTeXl5mDRpktnLcCMiIhASEoIJEybgl19+wbZt2/D6668jNjbWYvJEZC8c9WEOotvhGSuiekpOTgYADB061Kw8JSUFTz/9NIBrL8N1cnJCdHS02T0lJs7Ozti8eTOmTZsGrVaLli1bIiYmBvPnz7dWM4hkZw8PczjqjfnN4eEKW7WxtsdjYkVUT3K9DDcoKIg3hZNDsYeHORx9zDWHhyus3cbaPszBxIqIiGRjLw9z5M1+oEHtbKqaw8MVtmpjbR/maDaJFadqISJqPPb2MIejJh0mzeXhCmu2sbbHajaJFRERNZ7Y2FisWrUKGzdulB7mAK49xOHm5mb2MIe3tzfUajWmT59e48Mcixcvhk6n48McZHeYWBERUYPxYQ6ia5hYNRDnTyMi4sMcRCZ8jxURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTPhUoIPhU4pERES2wzNWRERERDJhYkVEREQkEyZWRERERDJhYkVEREQkE9683ow19Eb3G7c/uSiqoeEQERHZPZ6xIiIiaqCOr37Hp7IJABMrIiIiItnwUuANTH9tONplLf4VRUREZB08Y0VEREQkEyZWRERERDJp9pcC63KZjJfUalbTE4KWLq/yaUIiInJUPGNFREREJBMmVkREREQycfhLgfW5fMdLVdeY+kHlLLC4f/VyIiJqXI76tLoj4xkrIiIiIpkwsSIiIiKSCRMrIiIiIpkwsSIiIiKSicPfvE5NQ003vPNBASIiciQ8Y0VEREQkEyZWRERERDJplpcCOY1N01SX97Xw3S5ERNQU8YwVERERkUyYWBFRo+v46nc8+0tEzUKzvBRIjauxvkD5xUxERE2dzc5YJSUloWPHjnB1dUVYWBj27t1rq1CIbI7jwXpMZ8+YqDddHA9kz2ySWK1Zswbx8fGYM2cODhw4gN69eyMyMhJFRUW2CIfIpjgebo/JUPPB8UD2ziaJ1ZIlSzBlyhRMmjQJISEhWLFiBdzd3fHZZ5/ZIhxqYur7JXq77ZrqlzPHA9F1HA9k76yeWFVWViIvLw/h4eHXg3ByQnh4OHJycqwdDpFNcTzYrx5ztzW5JL0umuIfGhwP5AisfvP6+fPnUVVVBT8/P7NyPz8//P777xa30ev10Ov10nJJSQkA4OLFizAYDFK5wWBAeXk5WhicUGVUNEL0zVMLo0B5udEm/XrhwoXrcVwtq/X6G8trux4ALl26BAAQQtQ92HpozPEAVB8TNbX7VmrTb7XdR0O3b8g+5NqXHH3aFDT38SD3762hn3G59mtq44ULF6BUKm9bPywxU/r3noTh9QvSyvtVOQm8frex1m2s7X5Naoq31uNBWNlff/0lAIjs7Gyz8lmzZon+/ftb3GbOnDkCAH/4Y7WfU6dOWWM4cDzwxy5+OB74w5/rP7cbD1Y/Y9WmTRs4OzujsLDQrLywsBAajcbiNgkJCYiPj5eWjUYjLl68CB8fHygU18+glJaWIjAwEKdOnYJarW6cBjRDzalfhRC4dOkSAgICrHK8xhwPQPP63VlLc+pTjgf7wzY2ntqOB6snVi4uLggNDUVmZiZGjRoF4NpAyMzMRFxcnMVtVCoVVCqVWZmXl1eNx1Cr1Q77gbKl5tKvnp6eVjuWNcYD0Hx+d9bUXPqU48E+sY2NozbjwSYvCI2Pj0dMTAz69u2L/v37Y+nSpSgrK8OkSZNsEQ6RTXE8EF3H8UD2ziaJ1dixY3Hu3DnMnj0bOp0Offr0wdatW6vdsEjUHHA8EF3H8UD2zmZT2sTFxdV4are+VCoV5syZU+20MDUM+7XxNcZ4APi7awzs08bH8VB/bKPtKYSw0nO0RERERA7OZnMFEhERETkaJlZEREREMmFiRURERCQTh0qskpKS0LFjR7i6uiIsLAx79+61dUh2Ze7cuVAoFGY/wcHB0vqKigrExsbCx8cHrVq1QnR0dLUX+VHTwfEgr9uND2oa6vq5T0tLQ3BwMFxdXdGzZ09s2bLFSpHWXWJiIvr16wcPDw/4+vpi1KhRyM/Pv+U2qamp1T63rq6uVoq47uozzpra79BhEqs1a9YgPj4ec+bMwYEDB9C7d29ERkaiqKjI1qHZle7du+Ps2bPSzw8//CCtmzlzJjZt2oS0tDRkZWXhzJkzGDNmjA2jpZpwPDSOW40Psr26fu6zs7Px5JNPYvLkyfjpp58watQojBo1CocOHbJy5LWTlZWF2NhY5ObmIiMjAwaDARERESgrqz6P6o3UarXZ5/bPP/+0UsT1U5dx1iR/h3LM79QU9O/fX8TGxkrLVVVVIiAgQCQmJtowKvsyZ84c0bt3b4vriouLhVKpFGlpaVLZb7/9JgCInJwcK0VItcXxIL9bjQ9qGur6uX/iiSdEVFSUWVlYWJj45z//2ahxyqWoqEgAEFlZWTXWSUlJEZ6entYLqoHqOs6a4u/QIc5YVVZWIi8vD+Hh4VKZk5MTwsPDkZOTY8PI7M/Ro0cREBCAO+64A+PHj0dBQQEAIC8vDwaDwayPg4OD0aFDB/ZxE8Px0HhqGh9ke/X53Ofk5JjVB4DIyEi7GSclJSUAAG9v71vWu3z5MoKCghAYGIhHH30Uhw8ftkZ49VaXcdYUf4cOkVidP38eVVVV1d7M6+fnB51OZ6Oo7E9YWBhSU1OxdetWJCcn48SJExgyZAguXboEnU4HFxeXanNwsY+bHo6HxnGr8UG2V5/PvU6ns9txYjQaMWPGDAwaNAg9evSosV7Xrl3x2WefYePGjfjqq69gNBoxcOBAnD592orR1l5dx1lT/B3a7M3r1PSMHDlS+nevXr0QFhaGoKAgrF27Fm5ubjaMjMj2bjU+Jk+ebMPIqDmKjY3FoUOHbnufn1arhVarlZYHDhyIbt264aOPPsKCBQsaO8w6c4Rx5hBnrNq0aQNnZ+dqT6gVFhZCo9HYKCr75+XlhS5duuDYsWPQaDSorKxEcXGxWR32cdPD8WAdN44Psr36fO41Go1djpO4uDhs3rwZO3fuRPv27eu0rVKpxN133203n9vbjbOm+Dt0iMTKxcUFoaGhyMzMlMqMRiMyMzPNMnWqm8uXL+P48ePw9/dHaGgolEqlWR/n5+ejoKCAfdzEcDxYx43jg2yvPp97rVZrVh8AMjIymuw4EUIgLi4O69evx44dO9CpU6c676OqqgoHDx60m8/t7cZZk/wd2uy2eZmtXr1aqFQqkZqaKo4cOSKmTp0qvLy8hE6ns3VoduOll14Su3btEidOnBA//vijCA8PF23atBFFRUVCCCGee+450aFDB7Fjxw6xf/9+odVqhVartXHUZAnHg/xuNz7I9m73uZ8wYYJ49dVXpfo//vijaNGihXjnnXfEb7/9JubMmSOUSqU4ePCgrZpwS9OmTROenp5i165d4uzZs9JPeXm5VOfmNs6bN09s27ZNHD9+XOTl5Ylx48YJV1dXcfjwYVs04bZuN87s4XfoMImVEEIsW7ZMdOjQQbi4uIj+/fuL3NxcW4dkV8aOHSv8/f2Fi4uLaNeunRg7dqw4duyYtP7KlSvi+eefF61btxbu7u5i9OjR4uzZszaMmG6F40Fetxsf1DTc6nN/3333iZiYGLP6a9euFV26dBEuLi6ie/fu4rvvvrNyxLUHwOJPSkqKVOfmNs6YMUPqDz8/P/Hggw+KAwcOWD/4WrrdOLOH36FCCCFsd76MiIiIyHE4xD1WRERERE0BEysiIiIimTCxshOWJtI0/bz66qu2Do+IiIjAF4Tanfnz51d7xPZWb90lIiIi62FiZWdGjhyJvn37yra/srIytGzZUrb9ERERNWe8FOgA/vzzTzz//PPo2rUr3Nzc4OPjg8cffxwnT540q2e6nJiVlYXnn38evr6+Zm/t/f777zFkyBC0bNkSHh4eiIqKavKTdRIRETUlPGNlZ0pKSnD+/Hmzsn379iE7Oxvjxo1D+/btcfLkSSQnJ2Po0KE4cuQI3N3dzeo///zzaNu2LWbPno2ysjIAwJdffomYmBhERkbi7bffRnl5OZKTkzF48GD89NNP6Nixo7WaSEREZLeYWNmZ8PDwamXl5eV47LHHzMoefvhhaLVafPPNN5gwYYLZOm9vb2RmZsLZ2RnAtSkDXnjhBTz77LP4+OOPpXoxMTHo2rUrFi5caFZOREREljGxsjNJSUno0qWLWZmbm5v0b4PBgNLSUnTu3BleXl44cOBAtcRqypQpUlIFXJtXqbi4GE8++aTZ2TBnZ2eEhYVh586djdQaIiIix8LEys7079+/2s3rV65cQWJiIlJSUvDXX3/hxpfpl5SUVNvHzU8VHj16FAAwbNgwi8dUq9UNDZuIiKhZYGLlAKZPn46UlBTMmDEDWq0Wnp6eUCgUGDduHIxGY7X6N57hAiDV+fLLL6HRaKrVb9GCHxMiIqLa4DemA/j6668RExODd999VyqrqKhAcXFxrba/8847AQC+vr4W7+EiIiKi2uHrFhyAs7Mzbp5Le9myZaiqqqrV9pGRkVCr1Vi4cCEMBkO19efOnZMlTiIiIkfHM1YO4KGHHsKXX34JT09PhISEICcnB9u3b4ePj0+ttler1UhOTsaECRNwzz33YNy4cWjbti0KCgrw3XffYdCgQfjwww8buRVERET2j4mVA3j//ffh7OyMlStXoqKiAoMGDcL27dsRGRlZ63089dRTCAgIwKJFi/C///u/0Ov1aNeuHYYMGYJJkyY1YvRERESOQyFuvoZERERERPXCe6yIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZGKXLwg1Go04c+YMPDw8oFAobB0OORAhBC5duoSAgAA4OfHvDiIiqhu7TKzOnDmDwMBAW4dBDuzUqVNo3769rcMgIiI7Y5eJlYeHB4BrX35qtdpsncFgQHp6OiIiIqBUKm0RnuwcsU1A02xXaWkpAgMDpc8YERFRXdhlYmW6/KdWqy0mVu7u7lCr1U3my7qhHLFNQNNuFy8xExFRffAmEiIiIiKZMLEiIiIikgkTKyIiIiKZMLEiIiIikonDJlY95m5Dx1e/s3UYRERE1Iw4bGJFREREZG1MrIiIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCZMrIiIiIhkUqfEKjk5Gb169YJarYZarYZWq8X3338vra+oqEBsbCx8fHzQqlUrREdHo7Cw0GwfBQUFiIqKgru7O3x9fTFr1ixcvXpVntYQERER2VCdEqv27dtj0aJFyMvLw/79+zFs2DA8+uijOHz4MABg5syZ2LRpE9LS0pCVlYUzZ85gzJgx0vZVVVWIiopCZWUlsrOz8fnnnyM1NRWzZ8+Wt1VERERENtCiLpUffvhhs+W33noLycnJyM3NRfv27fHpp59i1apVGDZsGAAgJSUF3bp1Q25uLgYMGID09HQcOXIE27dvh5+fH/r06YMFCxbglVdewdy5c+Hi4iJfy4iIiIisrE6J1Y2qqqqQlpaGsrIyaLVa5OXlwWAwIDw8XKoTHByMDh06ICcnBwMGDEBOTg569uwJPz8/qU5kZCSmTZuGw4cP4+6777Z4LL1eD71eLy2XlpYCAAwGAwwGg1ld07LKSZgt2zNTGxyhLTdqiu1qSrEQEZH9qXNidfDgQWi1WlRUVKBVq1ZYv349QkJC8PPPP8PFxQVeXl5m9f38/KDT6QAAOp3OLKkyrTetq0liYiLmzZtXrTw9PR3u7u4Wt1nQ1wgA2LJlS63b1tRlZGTYOoRG0ZTaVV5ebusQiIjIjtU5seratSt+/vlnlJSU4Ouvv0ZMTAyysrIaIzZJQkIC4uPjpeXS0lIEBgYiIiICarXarK7BYEBGRgbe2O8EvVGBQ3MjGzU2azC1acSIEVAqlbYORzZNsV2ms6FERET1UefEysXFBZ07dwYAhIaGYt++fXj//fcxduxYVFZWori42OysVWFhITQaDQBAo9Fg7969ZvszPTVoqmOJSqWCSqWqVq5UKmv8QtYbFdBXKZrMF7YcbtVee9aU2tVU4iAiIvvU4PdYGY1G6PV6hIaGQqlUIjMzU1qXn5+PgoICaLVaAIBWq8XBgwdRVFQk1cnIyIBarUZISEhDQyEiIiKyqTqdsUpISMDIkSPRoUMHXLp0CatWrcKuXbuwbds2eHp6YvLkyYiPj4e3tzfUajWmT58OrVaLAQMGAAAiIiIQEhKCCRMmYPHixdDpdHj99dcRGxtr8YwUERERkT2pU2JVVFSEiRMn4uzZs/D09ESvXr2wbds2jBgxAgDw3nvvwcnJCdHR0dDr9YiMjMTy5cul7Z2dnbF582ZMmzYNWq0WLVu2RExMDObPny9vq4iIiIhsoE6J1aeffnrL9a6urkhKSkJSUlKNdYKCghzqST0iIiIiE84VSERERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCSTOiVWiYmJ6NevHzw8PODr64tRo0YhPz/frE5FRQViY2Ph4+ODVq1aITo6GoWFhWZ1CgoKEBUVBXd3d/j6+mLWrFm4evVqw1tDREREZEN1SqyysrIQGxuL3NxcZGRkwGAwICIiAmVlZVKdmTNnYtOmTUhLS0NWVhbOnDmDMWPGSOurqqoQFRWFyspKZGdn4/PPP0dqaipmz54tX6uIiIiIbKBFXSpv3brVbDk1NRW+vr7Iy8vDvffei5KSEnz66adYtWoVhg0bBgBISUlBt27dkJubiwEDBiA9PR1HjhzB9u3b4efnhz59+mDBggV45ZVXMHfuXLi4uMjXOiIiIiIratA9ViUlJQAAb29vAEBeXh4MBgPCw8OlOsHBwejQoQNycnIAADk5OejZsyf8/PykOpGRkSgtLcXhw4cbEg4RERGRTdXpjNWNjEYjZsyYgUGDBqFHjx4AAJ1OBxcXF3h5eZnV9fPzg06nk+rcmFSZ1pvWWaLX66HX66Xl0tJSAIDBYIDBYDCra1pWOQmzZXtmaoMjtOVGTbFdTSkWIiKyP/VOrGJjY3Ho0CH88MMPcsZjUWJiIubNm1etPD09He7u7ha3WdDXCADYsmVLo8ZmTRkZGbYOoVE0pXaVl5fbOgQiIrJj9Uqs4uLisHnzZuzevRvt27eXyjUaDSorK1FcXGx21qqwsBAajUaqs3fvXrP9mZ4aNNW5WUJCAuLj46Xl0tJSBAYGIiIiAmq12qyuwWBARkYG3tjvBL1RgUNzI+vTxCbF1KYRI0ZAqVTaOhzZNMV2mc6GEhER1UedEishBKZPn47169dj165d6NSpk9n60NBQKJVKZGZmIjo6GgCQn5+PgoICaLVaAIBWq8Vbb72FoqIi+Pr6Arh2xkKtViMkJMTicVUqFVQqVbVypVJZ4xey3qiAvkrRZL6w5XCr9tqzptSuphIHERHZpzolVrGxsVi1ahU2btwIDw8P6Z4oT09PuLm5wdPTE5MnT0Z8fDy8vb2hVqsxffp0aLVaDBgwAAAQERGBkJAQTJgwAYsXL4ZOp8Prr7+O2NhYi8kTERERkb2oU2KVnJwMABg6dKhZeUpKCp5++mkAwHvvvQcnJydER0dDr9cjMjISy5cvl+o6Oztj8+bNmDZtGrRaLVq2bImYmBjMnz+/YS0hIiIisrE6Xwq8HVdXVyQlJSEpKanGOkFBQQ51UzkRERERwLkCiYiIiGTj8IlVx1e/Q8dXv7N1GERERNQMOHxiRURERGQtTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZNLC1gFYS8dXv5P+fXJRlA0jISIiIkfFM1ZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMmFiRURERCQTJlZEREREMqlzYrV79248/PDDCAgIgEKhwIYNG8zWCyEwe/Zs+Pv7w83NDeHh4Th69KhZnYsXL2L8+PFQq9Xw8vLC5MmTcfny5QY1hIiIiMjW6pxYlZWVoXfv3khKSrK4fvHixfjggw+wYsUK7NmzBy1btkRkZCQqKiqkOuPHj8fhw4eRkZGBzZs3Y/fu3Zg6dWr9W0FERETUBLSo6wYjR47EyJEjLa4TQmDp0qV4/fXX8eijjwIAvvjiC/j5+WHDhg0YN24cfvvtN2zduhX79u1D3759AQDLli3Dgw8+iHfeeQcBAQENaA4RERGR7dQ5sbqVEydOQKfTITw8XCrz9PREWFgYcnJyMG7cOOTk5MDLy0tKqgAgPDwcTk5O2LNnD0aPHl1tv3q9Hnq9XlouLS0FABgMBhgMBrO6pmWVk6gxzpu3aepM8dpb3LfTFNvVlGIhIiL7I2tipdPpAAB+fn5m5X5+ftI6nU4HX19f8yBatIC3t7dU52aJiYmYN29etfL09HS4u7tb3GZBX2ONcW7ZsqXmRjRhGRkZtg6hUTSldpWXl9s6BCIismOyJlaNJSEhAfHx8dJyaWkpAgMDERERAbVabVbXYDAgIyMDb+x3gt6osLi/Q3MjGzVeuZnaNGLECCiVSluHI5um2C7T2VAiIqL6kDWx0mg0AIDCwkL4+/tL5YWFhejTp49Up6ioyGy7q1ev4uLFi9L2N1OpVFCpVNXKlUpljV/IeqMC+irLiVVT+RKvq1u11541pXY1lTiIiMg+yfoeq06dOkGj0SAzM1MqKy0txZ49e6DVagEAWq0WxcXFyMvLk+rs2LEDRqMRYWFhcoZDREREZFV1PmN1+fJlHDt2TFo+ceIEfv75Z3h7e6NDhw6YMWMG3nzzTdx1113o1KkT3njjDQQEBGDUqFEAgG7duuGBBx7AlClTsGLFChgMBsTFxWHcuHF8IpCIiIjsWp0Tq/379+P++++Xlk33PsXExCA1NRUvv/wyysrKMHXqVBQXF2Pw4MHYunUrXF1dpW1WrlyJuLg4DB8+HE5OToiOjsYHH3wgQ3OIiIiIbKfOidXQoUMhRM2vMlAoFJg/fz7mz59fYx1vb2+sWrWqrocmIiIiatI4VyARERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmEiRURERGRTJhYEREREcmkha0DsIWOr34n/fvkoigbRkJERESOhGesiIiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJkysbtDx1e/MbmwnIiIiqgsmVkREREQyYWJFREREJBMmVkREREQyaZYvCK0LvkyUiIiIaotnrIiIiIhkwsSKiIiISCZMrIiIiIhkwsSKiIiISCa8ed2Cml4SyhvZiYiI6FaafWLFN60TERGRXHgpkIiIiEgmNkuskpKS0LFjR7i6uiIsLAx79+61VShWY5qLkGfJiIiIHJNNLgWuWbMG8fHxWLFiBcLCwrB06VJERkYiPz8fvr6+tgip3mpKkm68B4uJFBERUfNgkzNWS5YswZQpUzBp0iSEhIRgxYoVcHd3x2effWaLcBwez5QRERFZh9XPWFVWViIvLw8JCQlSmZOTE8LDw5GTk2NxG71eD71eLy2XlJQAAC5evAiDwWBW12AwoLy8HC0MTqgyKhqhBdd0/tdaADV34IULF6R/t7hadsv1YYmZ0r/3JAyvVtfUpgsXLmDwO7ur1atpe1P5jTHeeFxbu7FdSqWy0Y93u34GgEuXLgEAhBCNHg8RETkehbDyN8iZM2fQrl07ZGdnQ6vVSuUvv/wysrKysGfPnmrbzJ07F/PmzbNmmNTMnTp1Cu3bt7d1GEREZGfs4nULCQkJiI+Pl5aNRiMuXrwIHx8fKBTmZ6VKS0sRGBiIU6dOQa1WWzvURuGIbQKaZruEELh06RICAgJsHQoREdkhqydWbdq0gbOzMwoLC83KCwsLodFoLG6jUqmgUqnMyry8vG55HLVa3WS+rOXiiG0Cml67PD09bR0CERHZKavfvO7i4oLQ0FBkZl6/38VoNCIzM9Ps0iARERGRvbHJpcD4+HjExMSgb9++6N+/P5YuXYqysjJMmjTJFuEQERERycImidXYsWNx7tw5zJ49GzqdDn369MHWrVvh5+fX4H2rVCrMmTOn2qVDe+aIbQIct11ERNR8Wf2pQCIiIiJHxbkCiYiIiGTCxIqIiIhIJkysiIiIiGTCxIqIiIhIJg6VWCUlJaFjx45wdXVFWFgY9u7da+uQapSYmIh+/frBw8MDvr6+GDVqFPLz883qVFRUIDY2Fj4+PmjVqhWio6OrvVi1oKAAUVFRcHd3h6+vL2bNmoWrV69asyk1WrRoERQKBWbMmCGV2XubiIiIbsVhEqs1a9YgPj4ec+bMwYEDB9C7d29ERkaiqKjI1qFZlJWVhdjYWOTm5iIjIwMGgwEREREoK7s+YfPMmTOxadMmpKWlISsrC2fOnMGYMWOk9VVVVYiKikJlZSWys7Px+eefIzU1FbNnz7ZFk8zs27cPH330EXr16mVWbs9tIiIiui3hIPr37y9iY2Ol5aqqKhEQECASExNtGFXtFRUVCQAiKytLCCFEcXGxUCqVIi0tTarz22+/CQAiJydHCCHEli1bhJOTk9DpdFKd5ORkoVarhV6vt24DbnDp0iVx1113iYyMDHHfffeJF198UQhh320iIiKqDYc4Y1VZWYm8vDyEh4dLZU5OTggPD0dOTo4NI6u9kpISAIC3tzcAIC8vDwaDwaxNwcHB6NChg9SmnJwc9OzZ0+zFqpGRkSgtLcXhw4etGL252NhYREVFmcUO2HebiIiIasMmb16X2/nz51FVVVXtze1+fn74/fffbRRV7RmNRsyYMQODBg1Cjx49AAA6nQ4uLi7VJpv28/ODTqeT6lhqs2mdLaxevRoHDhzAvn37qq2z1zYRERHVlkMkVvYuNjYWhw4dwg8//GDrUBrk1KlTePHFF5GRkQFXV1dbh0NERGR1DnEpsE2bNnB2dq72dFlhYSE0Go2NoqqduLg4bN68GTt37kT79u2lco1Gg8rKShQXF5vVv7FNGo3GYptN66wtLy8PRUVFuOeee9CiRQu0aNECWVlZ+OCDD9CiRQv4+fnZXZuIiIjqwiESKxcXF4SGhiIzM1MqMxqNyMzMhFartWFkNRNCIC4uDuvXr8eOHTvQqVMns/WhoaFQKpVmbcrPz0dBQYHUJq1Wi4MHD5o9+ZiRkQG1Wo2QkBDrNOQGw4cPx8GDB/Hzzz9LP3379sX48eOlf9tbm4iIiOrE1nfPy2X16tVCpVKJ1NRUceTIETF16lTh5eVl9nRZUzJt2jTh6ekpdu3aJc6ePSv9lJeXS3Wee+450aFDB7Fjxw6xf/9+odVqhVarldZfvXpV9OjRQ0RERIiff/5ZbN26VbRt21YkJCTYokkW3fhUoBCO0SYiIqKaOExiJYQQy5YtEx06dBAuLi6if//+Ijc319Yh1QiAxZ+UlBSpzpUrV8Tzzz8vWrduLdzd3cXo0aPF2bNnzfZz8uRJMXLkSOHm5ibatGkjXnrpJWEwGKzcmprdnFg5QpuIiIhqohBCCFueMSMiIiJyFA5xjxURERFRU8DEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgmTKyIiIiIZMLEioiIiEgm/w9BHIn0a1gZMAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 700x700 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualize your data using histogram\n",
    "#https://www.kaggle.com/code/ramsesmdlc/titanic-linear-regression-model\n",
    "\n",
    "df_train.hist(bins= 50, figsize = (7,7))\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.629630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.472826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.242363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass  Survived\n",
       "0       1  0.629630\n",
       "1       2  0.472826\n",
       "2       3  0.242363"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We can see that Pclass type 1 has the bigger change to survive and they has positive correlation to Survived\n",
    "df_train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female</td>\n",
       "      <td>0.742038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male</td>\n",
       "      <td>0.188908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex  Survived\n",
       "0  female  0.742038\n",
       "1    male  0.188908"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Female has the biggest change to stay alive. Sex has strong correlation to Survived feature\n",
    "df_train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.535885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.464286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.345395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SibSp  Survived\n",
       "1      1  0.535885\n",
       "2      2  0.464286\n",
       "0      0  0.345395\n",
       "3      3  0.250000\n",
       "4      4  0.166667\n",
       "5      5  0.000000\n",
       "6      8  0.000000"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#It's hard to see the correlation between these two variable. Then we will figure it out using correlation function in the next steps\n",
    "df_train[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Parch</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.550847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.343658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Parch  Survived\n",
       "3      3  0.600000\n",
       "1      1  0.550847\n",
       "2      2  0.500000\n",
       "0      0  0.343658\n",
       "5      5  0.200000\n",
       "4      4  0.000000\n",
       "6      6  0.000000"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Quite challenging to see whether Parch is effecting Survive or not\n",
    "df_train[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#To find the missing value in our data\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace missing value using mean value\n",
    "#https://stackoverflow.com/questions/18689823/pandas-dataframe-replace-nan-values-with-average-of-columns\n",
    "mean_value=df_train['Age'].mean()\n",
    "df_train['Age'].fillna(value=mean_value, inplace=True)\n",
    "\n",
    "#Encode object type into categorical and fill the missing value with mode\n",
    "#https://stackoverflow.com/questions/66056695/what-does-labelencoder-fit-do\n",
    "le.fit(df_train['Embarked'])\n",
    "le.transform(df_train['Embarked'])\n",
    "df_train['Embarked'] = le.transform(df_train['Embarked'])\n",
    "mode_value=df_train['Embarked'].mode()\n",
    "df_train['Embarked'].fillna(value=mode_value, inplace=True)\n",
    "\n",
    "#Encode object type into categorical and fill the missing value with mode\n",
    "#https://stackoverflow.com/questions/66056695/what-does-labelencoder-fit-do\n",
    "le.fit(df_train['Sex'])\n",
    "le.transform(df_train['Sex'])\n",
    "df_train['Sex'] = le.transform(df_train['Sex'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Because Cabin has too many missing value, we decide to uninclude this variable as one of our input. So we don't fill the missing value\n",
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Survived  Pclass  Sex        Age  SibSp  Parch     Fare  Embarked\n",
      "0           0       3    1  22.000000      1      0   7.2500         2\n",
      "1           1       1    0  38.000000      1      0  71.2833         0\n",
      "2           1       3    0  26.000000      0      0   7.9250         2\n",
      "3           1       1    0  35.000000      1      0  53.1000         2\n",
      "4           0       3    1  35.000000      0      0   8.0500         2\n",
      "..        ...     ...  ...        ...    ...    ...      ...       ...\n",
      "886         0       2    1  27.000000      0      0  13.0000         2\n",
      "887         1       1    0  19.000000      0      0  30.0000         2\n",
      "888         0       3    0  29.699118      1      2  23.4500         2\n",
      "889         1       1    1  26.000000      0      0  30.0000         0\n",
      "890         0       3    1  32.000000      0      0   7.7500         1\n",
      "\n",
      "[891 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "#Drop string data type and index value and use this data to make a correlation\n",
    "xtrain = df_train.drop(['PassengerId', 'Name', 'Cabin', 'Ticket'],axis=1)\n",
    "print(xtrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, -0.3384810359610152, -0.5433513806577559, -0.06980851528714312, -0.03532249888573563, 0.08162940708348347, 0.25730652238496243, -0.16351665142509866]\n"
     ]
    }
   ],
   "source": [
    "#We need to know the correlation between all possible inputs and Survived target.\n",
    "#Use pointbiserial to figure out categorical and numerical data type\n",
    "#We try to see correlation between Categorical : Survived vs Numerical : Age, SibSp, Parch, Fare\n",
    "#Only fare has highest number than 3 others numerical data type\n",
    "#https://stackoverflow.com/questions/67106853/how-to-do-point-biserial-correlation-for-multiple-columns-in-one-iteration\n",
    "\n",
    "corr_list = []\n",
    "y = df_train['Survived'].astype(float)\n",
    "\n",
    "for column in xtrain:\n",
    "    x=xtrain[column].astype(float)\n",
    "    corr = stats.pointbiserialr(list(x), list(y))\n",
    "    corr_list.append(corr[0])\n",
    "print(corr_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>0.34</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>0.54</td>\n",
       "      <td>0.13</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SibSp</th>\n",
       "      <td>0.19</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Parch</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare</th>\n",
       "      <td>0.45</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.82</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Survived  Pclass   Sex   Age  SibSp  Parch  Fare  Embarked\n",
       "Survived      1.00    0.34  0.54  0.17   0.19   0.16  0.45      0.17\n",
       "Pclass        0.34    1.00  0.13  0.31   0.15   0.02  0.82      0.26\n",
       "Sex           0.54    0.13  1.00  0.07   0.21   0.25  0.37      0.13\n",
       "Age           0.17    0.31  0.07  0.95   0.20   0.29  0.16      0.25\n",
       "SibSp         0.19    0.15  0.21  0.20   1.00   0.24  0.69      0.06\n",
       "Parch         0.16    0.02  0.25  0.29   0.24   1.00  0.35      0.00\n",
       "Fare          0.45    0.82  0.37  0.16   0.69   0.35  0.85      0.82\n",
       "Embarked      0.17    0.26  0.13  0.25   0.06   0.00  0.82      1.00"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use chi2 Creamer's V to figure out categorical and categorical data type\n",
    "#We try to see correlation between Categorical : Survived vs Categorical : Pclass, Sex, Embarked\n",
    "#Only Embarked shows really low correlation. Then we will drop Embarked\n",
    "#https://www.kaggle.com/code/chrisbss1/cramer-s-v-correlation-matrix/comments\n",
    "\n",
    "def cramers_V(var1,var2) :\n",
    "    crosstab = np.array(pd.crosstab(var1, var2, rownames=None, colnames=None)) # Crosstab building\n",
    "    chi2 = stats.chi2_contingency(crosstab)[0]\n",
    "    n = np.sum(crosstab)\n",
    "    phi2 = chi2 / n\n",
    "    r, k = crosstab.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)*2)/(n-1)\n",
    "    kcorr = k - ((k-1)*2)/(n-1)\n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "rows= []\n",
    "\n",
    "for var1 in xtrain:\n",
    "  col = []\n",
    "  for var2 in xtrain :\n",
    "    cramers =cramers_V(xtrain[var1], xtrain[var2]) # Cramer's V test\n",
    "    col.append(round(cramers,2)) # Keeping of the rounded value of the Cramer's V  \n",
    "  rows.append(col)\n",
    "  \n",
    "cramers_results = np.array(rows)\n",
    "df = pd.DataFrame(cramers_results, columns = xtrain.columns, index =xtrain.columns)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create New Data Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop unused variable from our data input\n",
    "xtrain = df_train.drop(['PassengerId','Survived', 'Name', 'SibSp', 'Age','Ticket', 'Parch', 'Cabin'],axis=1)\n",
    "ytrain = df_train['Survived']\n",
    "\n",
    "xtrain.to_csv('C:/Users/AMINA/Documents/belajar python/TITANIC/HW1/xtrain.csv', index=False)\n",
    "ytrain.to_csv('C:/Users/AMINA/Documents/belajar python/TITANIC/HW1/ytrain.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data using 5-fold Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1, Train set: 712, Test set:179\n",
      "Fold:2, Train set: 713, Test set:178\n",
      "Fold:3, Train set: 713, Test set:178\n",
      "Fold:4, Train set: 713, Test set:178\n",
      "Fold:5, Train set: 713, Test set:178\n"
     ]
    }
   ],
   "source": [
    "#Split training data using 5-fold cross validation\n",
    "from sklearn.model_selection import KFold\n",
    "cv = KFold(n_splits=5, random_state=1, shuffle=True)\n",
    "\n",
    "cnt = 1\n",
    "# split()  method generate indices to split data into training and test set.\n",
    "for train_index, test_index in cv.split(xtrain, ytrain):\n",
    "    print(f'Fold:{cnt}, Train set: {len(train_index)}, Test set:{len(test_index)}')\n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD  1\n",
      "TRAIN: [  0   1   4   5   7   9  10  11  12  14  15  18  20  21  22  24  25  26\n",
      "  27  28  29  30  31  32  33  35  36  37  38  39  40  41  42  43  44  45\n",
      "  46  48  51  52  53  54  55  57  58  61  62  63  64  66  67  68  70  71\n",
      "  72  74  75  76  77  78  79  80  82  83  86  87  88  89  91  92  93  95\n",
      "  96  97  98  99 100 102 103 104 105 108 109 110 112 113 114 115 116 118\n",
      " 120 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138\n",
      " 139 140 141 143 144 145 146 147 149 150 151 152 153 155 156 157 158 160\n",
      " 161 162 163 164 165 166 167 168 169 170 171 172 173 174 176 177 178 179\n",
      " 182 183 184 185 186 188 190 191 192 193 194 196 197 198 199 200 203 204\n",
      " 206 208 209 210 211 212 213 214 215 217 218 219 220 221 222 224 225 226\n",
      " 227 228 229 230 231 232 234 235 236 237 239 240 243 244 245 246 247 248\n",
      " 249 250 251 252 253 254 256 259 260 261 263 264 266 268 269 270 271 272\n",
      " 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 290 291\n",
      " 292 293 295 296 297 299 300 301 302 303 304 305 306 307 308 309 310 311\n",
      " 312 313 315 316 317 319 321 322 323 324 325 326 327 328 330 332 333 334\n",
      " 335 336 337 338 339 340 341 342 343 344 345 346 347 348 351 352 353 354\n",
      " 356 357 358 359 360 361 362 364 365 366 367 369 372 373 374 375 376 377\n",
      " 378 379 380 381 384 387 388 389 390 391 392 393 394 395 396 397 398 399\n",
      " 400 401 402 403 405 406 407 408 409 410 411 412 413 416 417 418 419 420\n",
      " 423 424 425 427 428 429 431 432 433 434 436 438 440 441 442 444 445 446\n",
      " 447 449 450 451 453 454 455 456 457 458 459 460 461 462 465 466 467 468\n",
      " 469 470 471 472 474 475 476 477 478 480 481 482 483 484 485 486 488 489\n",
      " 490 491 492 493 494 495 496 497 499 500 501 502 503 505 506 507 508 510\n",
      " 511 513 514 515 516 517 519 520 522 523 524 526 527 528 529 530 531 532\n",
      " 533 534 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551\n",
      " 552 553 555 556 557 558 559 560 561 562 563 564 566 567 568 569 570 571\n",
      " 572 573 575 576 577 578 579 580 581 582 583 586 587 588 590 591 592 593\n",
      " 595 596 601 602 603 604 605 606 607 608 609 611 612 613 614 615 616 617\n",
      " 618 620 621 624 626 627 628 630 631 632 633 634 635 636 637 638 639 641\n",
      " 642 645 646 647 648 650 651 652 653 654 655 656 657 660 661 663 664 665\n",
      " 666 667 668 669 670 672 673 674 675 676 677 678 679 682 683 684 685 687\n",
      " 688 689 690 691 693 694 695 696 697 698 699 700 701 704 705 706 707 709\n",
      " 711 712 714 715 716 717 718 720 721 722 723 725 727 728 730 731 732 734\n",
      " 735 736 738 740 741 744 745 746 747 748 749 750 751 752 753 754 755 756\n",
      " 758 759 760 761 762 763 764 765 767 768 770 771 773 774 775 776 777 778\n",
      " 779 780 781 782 783 784 785 786 787 788 791 792 793 798 799 800 801 802\n",
      " 803 804 805 806 807 808 809 810 812 814 816 818 820 823 824 825 826 829\n",
      " 831 832 833 834 835 836 837 838 839 840 842 843 844 846 847 849 850 851\n",
      " 852 853 854 855 856 857 858 860 861 863 864 867 869 870 872 873 874 875\n",
      " 876 879 880 881 883 884 886 888 889 890]\n",
      "TEST: [  2   3   6   8  13  16  17  19  23  34  47  49  50  56  59  60  65  69\n",
      "  73  81  84  85  90  94 101 106 107 111 117 119 121 142 148 154 159 175\n",
      " 180 181 187 189 195 201 202 205 207 216 223 233 238 241 242 255 257 258\n",
      " 262 265 267 289 294 298 314 318 320 329 331 349 350 355 363 368 370 371\n",
      " 382 383 385 386 404 414 415 421 422 426 430 435 437 439 443 448 452 463\n",
      " 464 473 479 487 498 504 509 512 518 521 525 535 554 565 574 584 585 589\n",
      " 594 597 598 599 600 610 619 622 623 625 629 640 643 644 649 658 659 662\n",
      " 671 680 681 686 692 702 703 708 710 713 719 724 726 729 733 737 739 742\n",
      " 743 757 766 769 772 789 790 794 795 796 797 811 813 815 817 819 821 822\n",
      " 827 828 830 841 845 848 859 862 865 866 868 871 877 878 882 885 887]\n",
      "FOLD  2\n",
      "TRAIN: [  1   2   3   4   5   6   7   8   9  10  12  13  15  16  17  18  19  20\n",
      "  21  22  23  24  25  26  27  28  30  32  33  34  36  37  38  39  42  43\n",
      "  44  47  48  49  50  51  52  53  55  56  59  60  63  64  65  67  69  70\n",
      "  71  72  73  75  77  78  79  80  81  83  84  85  86  87  90  93  94  96\n",
      "  97  98  99 100 101 102 105 106 107 109 110 111 112 113 115 117 119 121\n",
      " 122 123 124 126 127 128 129 130 131 133 136 137 138 140 141 142 143 144\n",
      " 145 146 148 149 150 151 152 153 154 155 156 157 158 159 162 163 164 165\n",
      " 166 167 168 169 170 171 174 175 176 177 178 179 180 181 182 183 186 187\n",
      " 188 189 190 191 192 193 195 196 197 198 199 200 201 202 203 205 206 207\n",
      " 208 209 210 211 212 213 215 216 217 219 220 221 222 223 225 227 229 230\n",
      " 231 232 233 234 235 236 237 238 240 241 242 243 249 250 251 252 253 254\n",
      " 255 256 257 258 259 260 261 262 263 264 265 266 267 269 270 271 272 275\n",
      " 276 278 279 280 281 282 283 284 287 288 289 290 292 293 294 295 296 297\n",
      " 298 300 302 303 304 306 308 310 312 313 314 315 316 317 318 319 320 321\n",
      " 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 342\n",
      " 343 347 348 349 350 351 352 353 355 356 357 358 359 360 361 362 363 365\n",
      " 367 368 369 370 371 373 374 376 377 378 379 380 381 382 383 384 385 386\n",
      " 387 389 390 391 393 394 395 396 397 398 399 400 401 402 403 404 405 406\n",
      " 407 409 410 413 414 415 416 417 418 419 420 421 422 423 424 425 426 428\n",
      " 430 431 432 434 435 436 437 438 439 440 441 442 443 444 445 447 448 450\n",
      " 451 452 454 455 456 457 458 459 460 461 462 463 464 465 466 468 469 470\n",
      " 471 472 473 475 476 477 478 479 482 483 485 486 487 488 489 490 491 492\n",
      " 495 497 498 499 500 501 502 503 504 505 508 509 511 512 513 514 515 517\n",
      " 518 519 520 521 523 524 525 526 531 532 533 534 535 536 537 539 540 541\n",
      " 542 544 545 546 547 548 552 554 555 556 557 559 560 561 562 564 565 566\n",
      " 567 568 569 572 574 575 577 579 580 582 583 584 585 586 587 588 589 590\n",
      " 591 592 594 595 596 597 598 599 600 601 602 603 604 606 607 608 609 610\n",
      " 611 612 615 616 617 618 619 620 621 622 623 625 626 627 628 629 630 631\n",
      " 633 636 638 640 641 642 643 644 645 646 647 648 649 650 651 653 655 656\n",
      " 658 659 660 661 662 663 665 666 667 668 669 670 671 672 673 675 679 680\n",
      " 681 682 685 686 687 689 691 692 693 694 695 696 697 698 699 700 701 702\n",
      " 703 705 706 707 708 710 711 712 713 714 715 716 717 718 719 720 722 723\n",
      " 724 725 726 727 728 729 732 733 734 735 737 738 739 742 743 745 746 747\n",
      " 749 751 752 753 754 755 756 757 758 759 760 762 763 764 766 767 768 769\n",
      " 770 771 772 774 776 777 778 779 780 781 782 783 784 785 786 787 789 790\n",
      " 792 793 794 795 796 797 798 799 801 804 805 806 807 808 809 810 811 812\n",
      " 813 814 815 817 819 820 821 822 823 825 826 827 828 829 830 833 834 835\n",
      " 836 837 838 839 840 841 843 845 846 847 848 849 851 853 854 855 856 857\n",
      " 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 877 878\n",
      " 879 880 881 882 883 884 885 886 887 889 890]\n",
      "TEST: [  0  11  14  29  31  35  40  41  45  46  54  57  58  61  62  66  68  74\n",
      "  76  82  88  89  91  92  95 103 104 108 114 116 118 120 125 132 134 135\n",
      " 139 147 160 161 172 173 184 185 194 204 214 218 224 226 228 239 244 245\n",
      " 246 247 248 268 273 274 277 285 286 291 299 301 305 307 309 311 339 340\n",
      " 341 344 345 346 354 364 366 372 375 388 392 408 411 412 427 429 433 446\n",
      " 449 453 467 474 480 481 484 493 494 496 506 507 510 516 522 527 528 529\n",
      " 530 538 543 549 550 551 553 558 563 570 571 573 576 578 581 593 605 613\n",
      " 614 624 632 634 635 637 639 652 654 657 664 674 676 677 678 683 684 688\n",
      " 690 704 709 721 730 731 736 740 741 744 748 750 761 765 773 775 788 791\n",
      " 800 802 803 816 818 824 831 832 842 844 850 852 874 875 876 888]\n",
      "FOLD  3\n",
      "TRAIN: [  0   1   2   3   6   7   8  10  11  13  14  15  16  17  19  20  21  22\n",
      "  23  24  25  26  28  29  31  32  34  35  36  37  40  41  43  44  45  46\n",
      "  47  48  49  50  51  53  54  55  56  57  58  59  60  61  62  63  64  65\n",
      "  66  68  69  70  71  72  73  74  75  76  77  79  81  82  83  84  85  86\n",
      "  87  88  89  90  91  92  93  94  95  96 100 101 103 104 105 106 107 108\n",
      " 109 111 112 113 114 115 116 117 118 119 120 121 123 125 126 127 129 130\n",
      " 131 132 134 135 136 137 138 139 140 141 142 143 144 145 147 148 149 150\n",
      " 151 152 154 155 158 159 160 161 163 164 166 167 169 170 171 172 173 174\n",
      " 175 176 178 180 181 182 183 184 185 187 188 189 190 193 194 195 196 198\n",
      " 199 201 202 204 205 206 207 208 209 210 211 212 213 214 215 216 218 219\n",
      " 222 223 224 226 228 229 230 231 233 234 235 238 239 240 241 242 243 244\n",
      " 245 246 247 248 249 251 252 253 254 255 256 257 258 262 263 264 265 266\n",
      " 267 268 269 270 271 273 274 275 276 277 278 279 281 282 285 286 287 288\n",
      " 289 290 291 294 295 296 297 298 299 300 301 302 303 305 307 308 309 311\n",
      " 313 314 315 316 317 318 319 320 321 322 324 325 326 327 328 329 330 331\n",
      " 332 336 337 338 339 340 341 342 344 345 346 348 349 350 351 354 355 356\n",
      " 357 362 363 364 365 366 367 368 369 370 371 372 375 376 380 381 382 383\n",
      " 384 385 386 387 388 390 391 392 393 398 401 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 420 421 422 423 424 425 426 427 429 430\n",
      " 431 433 435 437 438 439 440 441 443 444 446 448 449 450 451 452 453 454\n",
      " 456 458 460 461 462 463 464 465 466 467 468 469 470 471 473 474 475 476\n",
      " 477 478 479 480 481 484 485 487 489 490 492 493 494 496 497 498 499 500\n",
      " 503 504 505 506 507 508 509 510 512 513 514 515 516 518 520 521 522 524\n",
      " 525 526 527 528 529 530 532 533 534 535 536 538 539 542 543 545 547 548\n",
      " 549 550 551 553 554 556 557 558 559 561 562 563 564 565 566 568 569 570\n",
      " 571 572 573 574 575 576 577 578 580 581 582 583 584 585 586 588 589 590\n",
      " 591 593 594 595 596 597 598 599 600 605 606 609 610 611 613 614 615 616\n",
      " 617 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635\n",
      " 637 639 640 642 643 644 645 646 648 649 651 652 653 654 655 656 657 658\n",
      " 659 660 662 664 665 668 671 673 674 675 676 677 678 680 681 683 684 685\n",
      " 686 687 688 689 690 691 692 693 694 695 696 702 703 704 705 706 707 708\n",
      " 709 710 711 712 713 714 715 717 719 720 721 722 724 726 728 729 730 731\n",
      " 732 733 734 735 736 737 738 739 740 741 742 743 744 747 748 749 750 751\n",
      " 752 753 754 756 757 758 759 760 761 762 764 765 766 767 768 769 770 771\n",
      " 772 773 774 775 776 777 778 780 783 784 785 786 787 788 789 790 791 792\n",
      " 793 794 795 796 797 798 799 800 801 802 803 804 806 807 811 812 813 815\n",
      " 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833\n",
      " 834 835 836 838 840 841 842 843 844 845 846 847 848 849 850 852 854 856\n",
      " 859 860 861 862 863 865 866 867 868 869 870 871 872 874 875 876 877 878\n",
      " 879 880 881 882 883 885 886 887 888 889 890]\n",
      "TEST: [  4   5   9  12  18  27  30  33  38  39  42  52  67  78  80  97  98  99\n",
      " 102 110 122 124 128 133 146 153 156 157 162 165 168 177 179 186 191 192\n",
      " 197 200 203 217 220 221 225 227 232 236 237 250 259 260 261 272 280 283\n",
      " 284 292 293 304 306 310 312 323 333 334 335 343 347 352 353 358 359 360\n",
      " 361 373 374 377 378 379 389 394 395 396 397 399 400 402 403 419 428 432\n",
      " 434 436 442 445 447 455 457 459 472 482 483 486 488 491 495 501 502 511\n",
      " 517 519 523 531 537 540 541 544 546 552 555 560 567 579 587 592 601 602\n",
      " 603 604 607 608 612 618 636 638 641 647 650 661 663 666 667 669 670 672\n",
      " 679 682 697 698 699 700 701 716 718 723 725 727 745 746 755 763 779 781\n",
      " 782 805 808 809 810 814 837 839 851 853 855 857 858 864 873 884]\n",
      "FOLD  4\n",
      "TRAIN: [  0   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  22  23  25  26  27  29  30  31  33  34  35  37  38  39  40  41\n",
      "  42  43  45  46  47  49  50  52  54  56  57  58  59  60  61  62  64  65\n",
      "  66  67  68  69  71  72  73  74  75  76  77  78  80  81  82  84  85  86\n",
      "  87  88  89  90  91  92  94  95  96  97  98  99 101 102 103 104 106 107\n",
      " 108 110 111 114 116 117 118 119 120 121 122 124 125 126 128 129 130 132\n",
      " 133 134 135 139 140 141 142 144 146 147 148 149 151 152 153 154 155 156\n",
      " 157 159 160 161 162 165 166 168 170 172 173 175 176 177 178 179 180 181\n",
      " 183 184 185 186 187 189 190 191 192 193 194 195 196 197 198 200 201 202\n",
      " 203 204 205 207 209 210 214 215 216 217 218 220 221 223 224 225 226 227\n",
      " 228 231 232 233 235 236 237 238 239 241 242 243 244 245 246 247 248 250\n",
      " 252 253 254 255 257 258 259 260 261 262 263 264 265 266 267 268 269 272\n",
      " 273 274 276 277 278 279 280 281 282 283 284 285 286 288 289 291 292 293\n",
      " 294 297 298 299 301 302 304 305 306 307 309 310 311 312 313 314 316 317\n",
      " 318 319 320 321 323 327 329 331 332 333 334 335 336 338 339 340 341 343\n",
      " 344 345 346 347 348 349 350 352 353 354 355 356 357 358 359 360 361 363\n",
      " 364 366 367 368 369 370 371 372 373 374 375 377 378 379 381 382 383 385\n",
      " 386 387 388 389 390 392 393 394 395 396 397 398 399 400 401 402 403 404\n",
      " 405 408 411 412 414 415 416 418 419 421 422 426 427 428 429 430 431 432\n",
      " 433 434 435 436 437 439 442 443 445 446 447 448 449 452 453 454 455 456\n",
      " 457 459 461 463 464 466 467 468 469 470 471 472 473 474 476 478 479 480\n",
      " 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 521 522 523 525 527 528 529 530 531 532 534 535 537 538 540 541 542\n",
      " 543 544 546 548 549 550 551 552 553 554 555 558 560 561 562 563 564 565\n",
      " 566 567 568 569 570 571 572 573 574 576 578 579 580 581 583 584 585 586\n",
      " 587 588 589 592 593 594 595 597 598 599 600 601 602 603 604 605 606 607\n",
      " 608 609 610 612 613 614 615 616 618 619 621 622 623 624 625 626 627 629\n",
      " 630 632 633 634 635 636 637 638 639 640 641 643 644 645 647 648 649 650\n",
      " 652 653 654 655 657 658 659 660 661 662 663 664 665 666 667 668 669 670\n",
      " 671 672 674 676 677 678 679 680 681 682 683 684 686 688 690 692 693 694\n",
      " 695 697 698 699 700 701 702 703 704 706 707 708 709 710 712 713 714 715\n",
      " 716 718 719 721 722 723 724 725 726 727 728 729 730 731 732 733 736 737\n",
      " 738 739 740 741 742 743 744 745 746 748 749 750 751 752 753 755 757 758\n",
      " 760 761 762 763 765 766 767 769 771 772 773 774 775 777 778 779 781 782\n",
      " 788 789 790 791 792 793 794 795 796 797 800 801 802 803 805 808 809 810\n",
      " 811 812 813 814 815 816 817 818 819 820 821 822 823 824 826 827 828 829\n",
      " 830 831 832 833 834 835 837 838 839 840 841 842 843 844 845 847 848 849\n",
      " 850 851 852 853 854 855 857 858 859 862 864 865 866 868 871 873 874 875\n",
      " 876 877 878 879 880 882 884 885 886 887 888]\n",
      "TEST: [  1  21  24  28  32  36  44  48  51  53  55  63  70  79  83  93 100 105\n",
      " 109 112 113 115 123 127 131 136 137 138 143 145 150 158 163 164 167 169\n",
      " 171 174 182 188 199 206 208 211 212 213 219 222 229 230 234 240 249 251\n",
      " 256 270 271 275 287 290 295 296 300 303 308 315 322 324 325 326 328 330\n",
      " 337 342 351 362 365 376 380 384 391 406 407 409 410 413 417 420 423 424\n",
      " 425 438 440 441 444 450 451 458 460 462 465 475 477 499 500 520 524 526\n",
      " 533 536 539 545 547 556 557 559 575 577 582 590 591 596 611 617 620 628\n",
      " 631 642 646 651 656 673 675 685 687 689 691 696 705 711 717 720 734 735\n",
      " 747 754 756 759 764 768 770 776 780 783 784 785 786 787 798 799 804 806\n",
      " 807 825 836 846 856 860 861 863 867 869 870 872 881 883 889 890]\n",
      "FOLD  5\n",
      "TRAIN: [  0   1   2   3   4   5   6   8   9  11  12  13  14  16  17  18  19  21\n",
      "  23  24  27  28  29  30  31  32  33  34  35  36  38  39  40  41  42  44\n",
      "  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62\n",
      "  63  65  66  67  68  69  70  73  74  76  78  79  80  81  82  83  84  85\n",
      "  88  89  90  91  92  93  94  95  97  98  99 100 101 102 103 104 105 106\n",
      " 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124\n",
      " 125 127 128 131 132 133 134 135 136 137 138 139 142 143 145 146 147 148\n",
      " 150 153 154 156 157 158 159 160 161 162 163 164 165 167 168 169 171 172\n",
      " 173 174 175 177 179 180 181 182 184 185 186 187 188 189 191 192 194 195\n",
      " 197 199 200 201 202 203 204 205 206 207 208 211 212 213 214 216 217 218\n",
      " 219 220 221 222 223 224 225 226 227 228 229 230 232 233 234 236 237 238\n",
      " 239 240 241 242 244 245 246 247 248 249 250 251 255 256 257 258 259 260\n",
      " 261 262 265 267 268 270 271 272 273 274 275 277 280 283 284 285 286 287\n",
      " 289 290 291 292 293 294 295 296 298 299 300 301 303 304 305 306 307 308\n",
      " 309 310 311 312 314 315 318 320 322 323 324 325 326 328 329 330 331 333\n",
      " 334 335 337 339 340 341 342 343 344 345 346 347 349 350 351 352 353 354\n",
      " 355 358 359 360 361 362 363 364 365 366 368 370 371 372 373 374 375 376\n",
      " 377 378 379 380 382 383 384 385 386 388 389 391 392 394 395 396 397 399\n",
      " 400 402 403 404 406 407 408 409 410 411 412 413 414 415 417 419 420 421\n",
      " 422 423 424 425 426 427 428 429 430 432 433 434 435 436 437 438 439 440\n",
      " 441 442 443 444 445 446 447 448 449 450 451 452 453 455 457 458 459 460\n",
      " 462 463 464 465 467 472 473 474 475 477 479 480 481 482 483 484 486 487\n",
      " 488 491 493 494 495 496 498 499 500 501 502 504 506 507 509 510 511 512\n",
      " 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 533 535\n",
      " 536 537 538 539 540 541 543 544 545 546 547 549 550 551 552 553 554 555\n",
      " 556 557 558 559 560 563 565 567 570 571 573 574 575 576 577 578 579 581\n",
      " 582 584 585 587 589 590 591 592 593 594 596 597 598 599 600 601 602 603\n",
      " 604 605 607 608 610 611 612 613 614 617 618 619 620 622 623 624 625 628\n",
      " 629 631 632 634 635 636 637 638 639 640 641 642 643 644 646 647 649 650\n",
      " 651 652 654 656 657 658 659 661 662 663 664 666 667 669 670 671 672 673\n",
      " 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691\n",
      " 692 696 697 698 699 700 701 702 703 704 705 708 709 710 711 713 716 717\n",
      " 718 719 720 721 723 724 725 726 727 729 730 731 733 734 735 736 737 739\n",
      " 740 741 742 743 744 745 746 747 748 750 754 755 756 757 759 761 763 764\n",
      " 765 766 768 769 770 772 773 775 776 779 780 781 782 783 784 785 786 787\n",
      " 788 789 790 791 794 795 796 797 798 799 800 802 803 804 805 806 807 808\n",
      " 809 810 811 813 814 815 816 817 818 819 821 822 824 825 827 828 830 831\n",
      " 832 836 837 839 841 842 844 845 846 848 850 851 852 853 855 856 857 858\n",
      " 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876\n",
      " 877 878 881 882 883 884 885 887 888 889 890]\n",
      "TEST: [  7  10  15  20  22  25  26  37  43  64  71  72  75  77  86  87  96 126\n",
      " 129 130 140 141 144 149 151 152 155 166 170 176 178 183 190 193 196 198\n",
      " 209 210 215 231 235 243 252 253 254 263 264 266 269 276 278 279 281 282\n",
      " 288 297 302 313 316 317 319 321 327 332 336 338 348 356 357 367 369 381\n",
      " 387 390 393 398 401 405 416 418 431 454 456 461 466 468 469 470 471 476\n",
      " 478 485 489 490 492 497 503 505 508 513 514 515 532 534 542 548 561 562\n",
      " 564 566 568 569 572 580 583 586 588 595 606 609 615 616 621 626 627 630\n",
      " 633 645 648 653 655 660 665 668 693 694 695 706 707 712 714 715 722 728\n",
      " 732 738 749 751 752 753 758 760 762 767 771 774 777 778 792 793 801 812\n",
      " 820 823 826 829 833 834 835 838 840 843 847 849 854 879 880 886]\n"
     ]
    }
   ],
   "source": [
    "Path = r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.'\n",
    "fold_no = 1\n",
    "for train_index, test_index in cv.split(xtrain, ytrain):\n",
    "    print(\"FOLD \",fold_no)\n",
    "    print(\"TRAIN:\", train_index)\n",
    "    print(\"TEST:\", test_index)  \n",
    "\n",
    "    Xtrain = xtrain.iloc[train_index, :]\n",
    "    Xtest = xtrain.iloc[test_index, :]\n",
    "    Ytrain = ytrain[train_index]\n",
    "    Ytest = ytrain[test_index]\n",
    "\n",
    "    Xtrain.to_csv(Path + 'xtrain' + str(fold_no) + '.csv',index=False)\n",
    "    Xtest.to_csv(Path + 'xtest' + str(fold_no) + '.csv',index=False)\n",
    "    Ytrain.to_csv(Path + 'ytrain' + str(fold_no) + '.csv',index=False)\n",
    "    Ytest.to_csv(Path + 'ytest' + str(fold_no) + '.csv',index=False)\n",
    "\n",
    "    fold_no += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainfold1 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtrain1.csv')\n",
    "xtrainfold2 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtrain2.csv')\n",
    "xtrainfold3 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtrain3.csv')\n",
    "xtrainfold4 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtrain4.csv')\n",
    "xtrainfold5 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtrain5.csv')\n",
    "\n",
    "xtestfold1 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtest1.csv')\n",
    "xtestfold2 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtest2.csv')\n",
    "xtestfold3 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtest3.csv')\n",
    "xtestfold4 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtest4.csv')\n",
    "xtestfold5 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.xtest5.csv')\n",
    "\n",
    "ytrainfold1 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytrain1.csv')\n",
    "ytrainfold2 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytrain2.csv')\n",
    "ytrainfold3 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytrain3.csv')\n",
    "ytrainfold4 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytrain4.csv')\n",
    "ytrainfold5 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytrain5.csv')\n",
    "\n",
    "ytestfold1 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytest1.csv')\n",
    "ytestfold2 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytest2.csv')\n",
    "ytestfold3 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytest3.csv')\n",
    "ytestfold4 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytest4.csv')\n",
    "ytestfold5 = pd.read_csv(r'C:\\Users\\AMINA\\Documents\\belajar python\\TITANIC\\HW1\\fold\\.ytest5.csv')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Logistic Regression Classification\n",
    "#https://realpython.com/logistic-regression-python/#logistic-regression-python-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       106\n",
      "           1       0.76      0.68      0.72        73\n",
      "\n",
      "    accuracy                           0.78       179\n",
      "   macro avg       0.78      0.77      0.77       179\n",
      "weighted avg       0.78      0.78      0.78       179\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR1 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "LR1.fit(xtrainfold1, ytrainfold1)\n",
    "ypredLR1 = LR1.predict(xtestfold1)\n",
    "print(classification_report(ytestfold1, ypredLR1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.81      0.77       101\n",
      "           1       0.71      0.60      0.65        77\n",
      "\n",
      "    accuracy                           0.72       178\n",
      "   macro avg       0.72      0.70      0.71       178\n",
      "weighted avg       0.72      0.72      0.72       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR2 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "LR2.fit(xtrainfold2, ytrainfold2)\n",
    "ypredLR2 = LR2.predict(xtestfold2)\n",
    "print(classification_report(ytestfold2, ypredLR2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.82      0.82       109\n",
      "           1       0.71      0.71      0.71        69\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.76      0.76      0.76       178\n",
      "weighted avg       0.78      0.78      0.78       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR3 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "LR3.fit(xtrainfold3, ytrainfold3)\n",
    "ypredLR3 = LR3.predict(xtestfold3)\n",
    "print(classification_report(ytestfold3, ypredLR3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.84      0.86       116\n",
      "           1       0.73      0.77      0.75        62\n",
      "\n",
      "    accuracy                           0.82       178\n",
      "   macro avg       0.80      0.81      0.80       178\n",
      "weighted avg       0.82      0.82      0.82       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LR4 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "LR4.fit(xtrainfold4, ytrainfold4)\n",
    "ypredLR4 = LR4.predict(xtestfold4)\n",
    "print(classification_report(ytestfold4, ypredLR4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       117\n",
      "           1       0.71      0.69      0.70        61\n",
      "\n",
      "    accuracy                           0.80       178\n",
      "   macro avg       0.78      0.77      0.77       178\n",
      "weighted avg       0.80      0.80      0.80       178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\validation.py:1111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "LR5 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
    "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
    "                   n_jobs=None, penalty='l2',\n",
    "                   random_state=0, solver='liblinear', tol=0.0001, verbose=0,\n",
    "                   warm_start=False)\n",
    "\n",
    "LR5.fit(xtrainfold5, ytrainfold5)\n",
    "ypredLR5 = LR5.predict(xtestfold5)\n",
    "print(classification_report(ytestfold5, ypredLR5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Temp\\ipykernel_22832\\895330965.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF1.fit(xtrainfold1, ytrainfold1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.92      0.85       106\n",
      "           1       0.85      0.63      0.72        73\n",
      "\n",
      "    accuracy                           0.80       179\n",
      "   macro avg       0.82      0.78      0.79       179\n",
      "weighted avg       0.81      0.80      0.80       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF1 = RandomForestClassifier(criterion='gini', max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                             bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                             random_state=None, verbose=0, warm_start=False, \n",
    "                             class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "RF1.fit(xtrainfold1, ytrainfold1)\n",
    "ypredRF1 = RF1.predict(xtestfold1)\n",
    "print(classification_report(ytestfold1, ypredRF1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Temp\\ipykernel_22832\\3341806088.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF2.fit(xtrainfold2, ytrainfold2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.86      0.81       101\n",
      "           1       0.78      0.66      0.72        77\n",
      "\n",
      "    accuracy                           0.78       178\n",
      "   macro avg       0.78      0.76      0.77       178\n",
      "weighted avg       0.78      0.78      0.77       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF2 = RandomForestClassifier(criterion='gini', max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                             bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                             random_state=None, verbose=0, warm_start=False, \n",
    "                             class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "RF2.fit(xtrainfold2, ytrainfold2)\n",
    "ypredRF2 = RF2.predict(xtestfold2)\n",
    "print(classification_report(ytestfold2, ypredRF2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Temp\\ipykernel_22832\\780552014.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF3.fit(xtrainfold3, ytrainfold3)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.91      0.86       109\n",
      "           1       0.82      0.67      0.74        69\n",
      "\n",
      "    accuracy                           0.81       178\n",
      "   macro avg       0.82      0.79      0.80       178\n",
      "weighted avg       0.82      0.81      0.81       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF3 = RandomForestClassifier(criterion='gini', max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                             bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                             random_state=None, verbose=0, warm_start=False, \n",
    "                             class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "RF3.fit(xtrainfold3, ytrainfold3)\n",
    "ypredRF3 = RF3.predict(xtestfold3)\n",
    "print(classification_report(ytestfold3, ypredRF3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Temp\\ipykernel_22832\\725818436.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF4.fit(xtrainfold4, ytrainfold4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.90      0.88       116\n",
      "           1       0.79      0.73      0.76        62\n",
      "\n",
      "    accuracy                           0.84       178\n",
      "   macro avg       0.82      0.81      0.82       178\n",
      "weighted avg       0.84      0.84      0.84       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF4 = RandomForestClassifier(criterion='gini', max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                             bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                             random_state=None, verbose=0, warm_start=False, \n",
    "                             class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "RF4.fit(xtrainfold4, ytrainfold4)\n",
    "ypredRF4 = RF4.predict(xtestfold4)\n",
    "print(classification_report(ytestfold4, ypredRF4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AMINA\\AppData\\Local\\Temp\\ipykernel_22832\\1933291975.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  RF5.fit(xtrainfold5, ytrainfold5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.84      0.86       117\n",
      "           1       0.72      0.80      0.76        61\n",
      "\n",
      "    accuracy                           0.83       178\n",
      "   macro avg       0.81      0.82      0.81       178\n",
      "weighted avg       0.83      0.83      0.83       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "RF5 = RandomForestClassifier(criterion='gini', max_depth=None, \n",
    "                             min_samples_split=2, min_samples_leaf=1, \n",
    "                             min_weight_fraction_leaf=0.0, max_features='sqrt', \n",
    "                             max_leaf_nodes=None, min_impurity_decrease=0.0, \n",
    "                             bootstrap=True, oob_score=False, n_jobs=None, \n",
    "                             random_state=None, verbose=0, warm_start=False, \n",
    "                             class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
    "RF5.fit(xtrainfold5, ytrainfold5)\n",
    "ypredRF5 = RF5.predict(xtestfold5)\n",
    "print(classification_report(ytestfold5, ypredRF5))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest - Hyperparameter Tuning\n",
    "\n",
    "To find best parameter on line 141-146 we use this code from this source\n",
    "https://www.datacamp.com/tutorial/random-forests-classifier-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {'n_estimators': randint(50,500),\n",
    "              'max_depth': randint(1,20)}\n",
    "\n",
    "# Create a random forest classifier\n",
    "RF = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 9, 'n_estimators': 356}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.98      0.91       106\n",
      "           1       0.96      0.75      0.85        73\n",
      "\n",
      "    accuracy                           0.89       179\n",
      "   macro avg       0.91      0.87      0.88       179\n",
      "weighted avg       0.90      0.89      0.89       179\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use random search to find the best hyperparameters\n",
    "rand_search1 = RandomizedSearchCV(RF, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search1.fit(xtrain, ytrain)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf1 = rand_search1.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search1.best_params_)\n",
    "\n",
    "# Generate predictions with the best model\n",
    "ypredRFtun1 = best_rf1.predict(xtestfold1)\n",
    "print(classification_report(ytestfold1, ypredRFtun1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 7, 'n_estimators': 454}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.96      0.87       101\n",
      "           1       0.93      0.68      0.78        77\n",
      "\n",
      "    accuracy                           0.84       178\n",
      "   macro avg       0.86      0.82      0.83       178\n",
      "weighted avg       0.85      0.84      0.83       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use random search to find the best hyperparameters\n",
    "rand_search2 = RandomizedSearchCV(RF, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search2.fit(xtrain, ytrain)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf2 = rand_search2.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search2.best_params_)\n",
    "\n",
    "# Generate predictions with the best model\n",
    "ypredRFtun2 = best_rf2.predict(xtestfold2)\n",
    "print(classification_report(ytestfold2, ypredRFtun2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 6, 'n_estimators': 331}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.89       109\n",
      "           1       0.94      0.68      0.79        69\n",
      "\n",
      "    accuracy                           0.86       178\n",
      "   macro avg       0.88      0.83      0.84       178\n",
      "weighted avg       0.87      0.86      0.85       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use random search to find the best hyperparameters\n",
    "rand_search3 = RandomizedSearchCV(RF, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search3.fit(xtrain, ytrain)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf3 = rand_search3.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search3.best_params_)\n",
    "\n",
    "# Generate predictions with the best model\n",
    "ypredRFtun3 = best_rf3.predict(xtestfold3)\n",
    "print(classification_report(ytestfold3, ypredRFtun3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 6, 'n_estimators': 270}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.97      0.92       116\n",
      "           1       0.94      0.74      0.83        62\n",
      "\n",
      "    accuracy                           0.89       178\n",
      "   macro avg       0.91      0.86      0.88       178\n",
      "weighted avg       0.90      0.89      0.89       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use random search to find the best hyperparameters\n",
    "rand_search4 = RandomizedSearchCV(RF, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search4.fit(xtrain, ytrain)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf4 = rand_search4.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search4.best_params_)\n",
    "\n",
    "# Generate predictions with the best model\n",
    "ypredRFtun4 = best_rf4.predict(xtestfold4)\n",
    "print(classification_report(ytestfold4, ypredRFtun4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 9, 'n_estimators': 101}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       117\n",
      "           1       0.86      0.84      0.85        61\n",
      "\n",
      "    accuracy                           0.90       178\n",
      "   macro avg       0.89      0.88      0.89       178\n",
      "weighted avg       0.90      0.90      0.90       178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use random search to find the best hyperparameters\n",
    "rand_search5 = RandomizedSearchCV(RF, \n",
    "                                 param_distributions = param_dist, \n",
    "                                 n_iter=5, \n",
    "                                 cv=5)\n",
    "\n",
    "# Fit the random search object to the data\n",
    "rand_search5.fit(xtrain, ytrain)\n",
    "\n",
    "# Create a variable for the best model\n",
    "best_rf5 = rand_search5.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:',  rand_search5.best_params_)\n",
    "\n",
    "# Generate predictions with the best model\n",
    "ypredRFtun5 = best_rf5.predict(xtestfold5)\n",
    "print(classification_report(ytestfold5, ypredRFtun5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Pclass  Sex      Fare  Embarked\n",
      "0         3    1    7.8292         1\n",
      "1         3    0    7.0000         2\n",
      "2         2    1    9.6875         1\n",
      "3         3    1    8.6625         2\n",
      "4         3    0   12.2875         2\n",
      "..      ...  ...       ...       ...\n",
      "413       3    1    8.0500         2\n",
      "414       1    0  108.9000         0\n",
      "415       3    1    7.2500         2\n",
      "416       3    1    8.0500         2\n",
      "417       3    1   22.3583         0\n",
      "\n",
      "[418 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "df_test=pd.read_csv(r'C:\\Users\\AMINA\\OneDrive\\S2\\S2\\Big Data Analysis and App\\week 1 -2\\test.csv')\n",
    "df_test=df_test.drop(['PassengerId', 'Name', 'SibSp', 'Age','Ticket', 'Parch','Cabin'], axis=1)\n",
    "\n",
    "le.fit(df_test['Sex'])\n",
    "le.transform(df_test['Sex'])\n",
    "df_test['Sex'] = le.transform(df_test['Sex'])\n",
    "\n",
    "le.fit(df_test['Embarked'])\n",
    "le.transform(df_test['Embarked'])\n",
    "df_test['Embarked'] = le.transform(df_test['Embarked'])\n",
    "\n",
    "mean_value=df_test['Fare'].mean()\n",
    "df_test['Fare'].fillna(value=mean_value, inplace=True)\n",
    "\n",
    "df_test.to_csv('C:/Users/AMINA/Documents/belajar python/TITANIC/HW1/testRF.csv', index=False)\n",
    "print(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass      0\n",
       "Sex         0\n",
       "Fare        0\n",
       "Embarked    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict New Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 1 1 0 0 1\n",
      " 1 0 0 0 0 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 1 1 1 0 0 1 1 0 1 0\n",
      " 1 1 0 1 0 1 0 1 0 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0\n",
      " 0 0 1 0 0 1 0 0 1 1 0 1 1 0 1 0 0 1 0 0 1 1 0 0 0 0 0 1 1 0 1 1 0 0 1 0 1\n",
      " 0 1 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0 1 1 0 0 1 0 1 0 0 0 0 1 1 0 1 0 1 0 1 0\n",
      " 1 0 1 1 0 1 0 0 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 0 1 0 1 1 1 0 1 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 1 0 0 0 1 1 0 1 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 1 0 0 0 0\n",
      " 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 1 0 1 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 1 1 1 1 1 1 0 1 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "ypredLR4 = LR4.predict(df_test)\n",
    "print(ypredLR4)\n",
    "pd.DataFrame(ypredLR4).to_csv('C:/Users/AMINA/Documents/belajar python/TITANIC/HW1/PredResult/LR4.csv', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 0 1 0 0 1 1 0 0 0 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 1 1 0 0 1 0 1 0 0\n",
      " 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 1 0 0 0 1 0 1 1 0 0 1 1 0 1 0\n",
      " 1 0 0 1 0 1 1 0 0 0 0 0 1 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0\n",
      " 1 1 0 1 0 0 1 1 1 1 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 1 0 0 1 1 0 1 0 1 0 1 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 0 1 0 0 0 0 1 0 0 1 0 1 0 1 0\n",
      " 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 1 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 0\n",
      " 1 1 0 0 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 1 1 0 0 0 1 0 1 0 0 1 0 1 1 0 1 0 0 0 1 0\n",
      " 0 1 0 0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0\n",
      " 0 1 0 1 1 0 0 1 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "ypredRF4 = RF4.predict(df_test)\n",
    "print(ypredRF4)\n",
    "pd.DataFrame(ypredRF4).to_csv('C:/Users/AMINA/Documents/belajar python/TITANIC/HW1/PredResult/RF4.csv', header=False, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0\n",
      " 0 1 0 1 0 1 0 1 0 1 0 1 0 0 1 0 0 0 1 0 0 1 0 1 0 1 1 0 0 0 1 1 1 1 1 1 1\n",
      " 1 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 1 0 1 0 0 0 1 1 0 1 0 1 1 0 1 0 1 0 1 0 0 0 1 0 1 1 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 1 0 1 1 0 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "ypredRFtun5 = best_rf5.predict(df_test)\n",
    "print(ypredRF5)\n",
    "pd.DataFrame(ypredRFtun5).to_csv('C:/Users/AMINA/Documents/belajar python/TITANIC/HW1/PredResult/RFtun5.csv', header=False, index=False)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
